{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033ba6d0",
   "metadata": {},
   "source": [
    "# 01 — Load and Prepare the Dataset\n",
    "\n",
    "In this notebook we turn the raw Seattle loop CSV dump into a clean,\n",
    "analysis-ready **time × detector** speed panel.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "- **Load the raw CSV** (`loop_20150101_20151231.csv`), assign column names, and\n",
    "  parse the `time` field.\n",
    "- **Pivot to a wide panel** where:\n",
    "  - Rows = 5-minute timestamps over 2015.\n",
    "  - Columns = detector IDs.\n",
    "  - Entries = speed readings (mph).\n",
    "- **Enforce a strict 5-minute grid**:\n",
    "  - Build a complete `date_range` from the first to last timestamp.\n",
    "  - Reindex the panel so that every 5-minute slot exists; missing rows become `NaN`.\n",
    "- **Clean unreliable raw values**:\n",
    "  - Treat `speed == 0` as missing and convert it to `NaN`.\n",
    "  - Detect “frozen” sensors that report the *exact* same speed for ≥60 minutes\n",
    "    and mark those flat segments as missing as well.\n",
    "\n",
    "Finally, we save the cleaned panel to disk in two formats:\n",
    "\n",
    "- `data/seattle_loop_clean.parquet`\n",
    "- `data/seattle_loop_clean.pkl`\n",
    "\n",
    "These files are the **canonical input** for all later notebooks (missingness EDA,\n",
    "blackout detection, and model training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f2f50b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6661c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataframe shape: (22211424, 6)\n",
      "Time span: 2015-01-01 00:00:00 → 2015-12-31 23:55:00\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1. Load raw CSV\n",
    "# -------------------------------\n",
    "\n",
    "# Read the CSV file containing loop detector data\n",
    "# There are no headers in file, so we provide column names explicitly\n",
    "df = pd.read_csv(\n",
    "    \"loop_20150101_20151231.csv\",\n",
    "    header=None,\n",
    "    names=[\"time\", \"detector\", \"direction\", \"speed\", \"volume\", \"occupancy\"]\n",
    ")\n",
    "\n",
    "# Convert 'time' to datetime and sort chronologically\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "df = df.sort_values(\"time\")\n",
    "\n",
    "print(\"Raw dataframe shape:\", df.shape)\n",
    "print(\"Time span:\", df[\"time\"].min(), \"→\", df[\"time\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38247bd2",
   "metadata": {},
   "source": [
    "### Pivot Data to Wide Format (Time × Detector) and Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da1b3064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 2. Pivot to time × detector panel\n",
    "# ----------------------------------\n",
    "\n",
    "# Reshape the DataFrame so that each row is a timestamp\n",
    "# and each column is a detector's speed reading\n",
    "wide = df.pivot_table(\n",
    "    index=\"time\",\n",
    "    columns=\"detector\",\n",
    "    values=\"speed\"\n",
    ").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42b0db01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide panel shape AFTER reindex: (105120, 147)\n",
      "Expected number of 5-min steps: 105120.0\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------\n",
    "# 3. Enforce strict 5-minute grid \n",
    "# ---------------------------------\n",
    "\n",
    "# Build a full 5-minute index from first to last timestamp\n",
    "full_index = pd.date_range(\n",
    "    start=wide.index.min(),\n",
    "    end=wide.index.max(),\n",
    "    freq=\"5min\"\n",
    ")\n",
    "\n",
    "# Reindex the panel so every 5-minute slot exists;\n",
    "# any missing timestamps become rows full of NaN\n",
    "wide = wide.reindex(full_index)\n",
    "wide.index.name = \"time\"  # keep index name clean\n",
    "\n",
    "print(\"Wide panel shape AFTER reindex:\", wide.shape)\n",
    "print(\"Expected number of 5-min steps:\", (full_index[-1] - full_index[0]) / pd.Timedelta(\"5min\") + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28810ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4. Clean speed values           # (zeros → NaN)\n",
    "# -------------------------------\n",
    "# Ensure we are working in float so NaNs are handled cleanly\n",
    "wide = wide.astype(float)\n",
    "\n",
    "# Replace speed = 0 (sensor failure / no data) with NaN\n",
    "wide = wide.replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64327e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction missing overall (after freezing): 0.051772771513476014\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# 5. Identify and mask frozen readings (> 1 hour)\n",
    "# -------------------------------------------------\n",
    "values = wide.to_numpy().astype(float)\n",
    "T, D = values.shape\n",
    "\n",
    "# Initialize a boolean mask of the same shape to mark frozen values\n",
    "frozen = np.zeros_like(values, dtype=bool)\n",
    "MAX_RUN = 12  # Define a frozen run as 12 or more identical values\n",
    "              # (12 × 5min = 60 minutes of unchanged readings)\n",
    "\n",
    "\n",
    "# Loop through each detector column\n",
    "for d in range(D):\n",
    "    col = values[:, d]  # Extract the column of speed values\n",
    "    run_val = col[0]    # Start by tracking the first value\n",
    "    run_start = 0       # Index where the current run begins\n",
    "\n",
    "    for t in range(1, T):\n",
    "        # Check if current value is the same as the previous one\n",
    "        # Also treats two NaNs in a row as \"equal\"\n",
    "        same = (col[t] == run_val) or (np.isnan(col[t]) and np.isnan(run_val))\n",
    "        if same:\n",
    "            continue  # Still in the same constant run — keep going\n",
    "\n",
    "        # value changed -> close current run\n",
    "        run_len = t - run_start\n",
    "        if run_len >= MAX_RUN and not np.isnan(run_val):\n",
    "            # If run was too long AND value wasn't already NaN → mark it as frozen\n",
    "            frozen[run_start:t, d] = True\n",
    "\n",
    "        # Start tracking the new value\n",
    "        run_val = col[t]\n",
    "        run_start = t\n",
    "\n",
    "    # Handle case where a run continues to the last time step\n",
    "    run_len = T - run_start\n",
    "    if run_len >= MAX_RUN and not np.isnan(run_val):\n",
    "        frozen[run_start:T, d] = True\n",
    "\n",
    "# After building the frozen mask, replace those entries with NaN\n",
    "# This treats long flatlines as missing data (likely sensor malfunction)\n",
    "values[frozen] = np.nan\n",
    "\n",
    "# Convert back to DataFrame with the same index/columns\n",
    "wide = pd.DataFrame(values, index=wide.index, columns=wide.columns)\n",
    "\n",
    "# Print final fraction of missing data (after frozen value masking)\n",
    "print(\"Fraction missing overall (after freezing):\",\n",
    "      np.mean(np.isnan(wide.to_numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca706d5",
   "metadata": {},
   "source": [
    "- Rows: 105,120 → exactly 365 days × 24 hours × 12 (5-min slots)\n",
    "- Cols: 147 detectors\n",
    "- Fraction missing overall: 0.0518 → about 5.2% of all (time, detector) cells are NaN  \n",
    "- 5.2% means ~800,000 missing datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efcbe79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved clean panel to:\n",
      "  - C:\\Users\\asust\\Desktop\\Modeling Information Blackouts\\data\\seattle_loop_clean.parquet\n",
      "  - C:\\Users\\asust\\Desktop\\Modeling Information Blackouts\\data\\seattle_loop_clean.pkl\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 6. Save clean panel            \n",
    "# -------------------------------\n",
    "\n",
    "# Make sure a data/ folder exists \n",
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "\n",
    "out_parquet = Path(\"data\") / \"seattle_loop_clean.parquet\"\n",
    "out_pickle  = Path(\"data\") / \"seattle_loop_clean.pkl\"\n",
    "\n",
    "# Overwrite any existing file\n",
    "wide.to_parquet(out_parquet, engine=\"pyarrow\")\n",
    "wide.to_pickle(out_pickle)\n",
    "\n",
    "print(\"Saved clean panel to:\")\n",
    "print(\"  -\", out_parquet.resolve())\n",
    "print(\"  -\", out_pickle.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5a2dc2",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This notebook converts the raw 2015 Seattle loop CSV file into a clean,\n",
    "regular time series panel suitable for modeling.\n",
    "\n",
    "- The final panel has **105,120 rows** (one full year of 5-minute intervals)\n",
    "  and **147 detectors**.\n",
    "- We standardize timestamps, pivot to a **time × detector** matrix, and enforce\n",
    "  a strict 5-minute grid.\n",
    "- We treat `0` speeds and long frozen runs (≥60 minutes of identical readings)\n",
    "  as missing and convert them to `NaN`.\n",
    "- The resulting dataset has a small but meaningful amount of missingness\n",
    "  (≈5% of all entries), with clear outages that we will analyze in the next\n",
    "  notebook.\n",
    "\n",
    "Outputs:\n",
    "\n",
    "- `data/seattle_loop_clean.parquet`\n",
    "- `data/seattle_loop_clean.pkl`\n",
    "\n",
    "These files are the starting point for **02 — Exploratory Data Analysis on Missingness**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

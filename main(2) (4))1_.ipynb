{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb99cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e92d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, log_loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import copy\n",
    "import inspect\n",
    "import pandas as pd\n",
    "\n",
    "import data_interface\n",
    "import mnar_blackout_lds\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac611d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "x_t, m_t, meta = data_interface.load_panel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecf39e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_windows = data_interface.get_eval_windows(\n",
    "    data_dir=\"data\",\n",
    "    manifest_name=\"evaluation_windows_mnar_weighted.parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddbd4fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def stratified_month_sampling(data, n_per_month, ts_key=\"blackout_start\"):\n",
    "    buckets = defaultdict(list)\n",
    "\n",
    "    for item in data:\n",
    "        ts = item[ts_key]\n",
    "        month_key = (ts.year, ts.month)\n",
    "        buckets[month_key].append(item)\n",
    "\n",
    "    result = []\n",
    "    for month_key, items in buckets.items():\n",
    "        if len(items) < n_per_month:\n",
    "            picks = random.choices(items, k=n_per_month)\n",
    "        else:\n",
    "            picks = random.sample(items, n_per_month)\n",
    "        result.extend(picks)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7067708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 1) Group evaluation windows by window_id and test_type/horizon\n",
    "#    (handles cases where there are multiple rows per window_id).\n",
    "# ------------------------------------------------------------------\n",
    "impute_by_id = {}\n",
    "forecast_1_by_id = {}\n",
    "forecast_3_by_id = {}\n",
    "forecast_6_by_id = {}\n",
    "\n",
    "for w in evaluation_windows:\n",
    "    wid = w[\"window_id\"]\n",
    "    if w[\"test_type\"] == \"impute\":\n",
    "        # If duplicates exist, last one wins – that's fine for eval.\n",
    "        impute_by_id[wid] = w\n",
    "    elif w[\"test_type\"] == \"forecast\":\n",
    "        h = int(w[\"horizon_steps\"])\n",
    "        if h == 1:\n",
    "            forecast_1_by_id[wid] = w\n",
    "        elif h == 3:\n",
    "            forecast_3_by_id[wid] = w\n",
    "        elif h == 6:\n",
    "            forecast_6_by_id[wid] = w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b03b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 2) Keep only window_ids that have impute + 1-step + 3-step + 6-step\n",
    "# ------------------------------------------------------------------\n",
    "common_ids = (\n",
    "    set(impute_by_id.keys())\n",
    "    & set(forecast_1_by_id.keys())\n",
    "    & set(forecast_3_by_id.keys())\n",
    "    & set(forecast_6_by_id.keys())\n",
    ")\n",
    "\n",
    "# Pool of impute windows that have all matching forecast horizons\n",
    "impute_windows_pool = [impute_by_id[wid] for wid in common_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5618994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 3) Sample *validation* impute windows (stratified by month)\n",
    "# ------------------------------------------------------------------\n",
    "impute_evaluation_windows_val = stratified_month_sampling(\n",
    "    impute_windows_pool,\n",
    "    n_per_month=25,\n",
    "    ts_key=\"blackout_start\",\n",
    ")\n",
    "\n",
    "# Preserve order: we now build *aligned* forecast lists\n",
    "val_ids_ordered = [w[\"window_id\"] for w in impute_evaluation_windows_val]\n",
    "\n",
    "forecast_1_evaluation_windows_val = [\n",
    "    forecast_1_by_id[wid] for wid in val_ids_ordered\n",
    "]\n",
    "forecast_3_evaluation_windows_val = [\n",
    "    forecast_3_by_id[wid] for wid in val_ids_ordered\n",
    "]\n",
    "forecast_6_evaluation_windows_val = [\n",
    "    forecast_6_by_id[wid] for wid in val_ids_ordered\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f5d8779",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(forecast_1_evaluation_windows_val) == len(impute_evaluation_windows_val)\n",
    "assert len(forecast_3_evaluation_windows_val) == len(impute_evaluation_windows_val)\n",
    "assert len(forecast_6_evaluation_windows_val) == len(impute_evaluation_windows_val)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Combined list used only for masking (deduped later)\n",
    "# ------------------------------------------------------------------\n",
    "evaluation_windows_val = (\n",
    "    forecast_1_evaluation_windows_val\n",
    "    + forecast_3_evaluation_windows_val\n",
    "    + forecast_6_evaluation_windows_val\n",
    "    + impute_evaluation_windows_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca03ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) Time features (used by diagnosis + seasonal baselines)\n",
    "# ============================================================\n",
    "def build_time_features(timestamps: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    timestamps: np.ndarray of pandas.Timestamp, shape (T,)\n",
    "    Returns: X_time, shape (T, 6)\n",
    "      [sin_hour, cos_hour, sin_dow, cos_dow, is_weekend, is_rush]\n",
    "    \"\"\"\n",
    "    ts = pd.to_datetime(timestamps)\n",
    "    hour = ts.hour.to_numpy()\n",
    "    dow = ts.dayofweek.to_numpy()  # Mon=0\n",
    "\n",
    "    hour_rad = 2.0 * np.pi * (hour / 24.0)\n",
    "    dow_rad = 2.0 * np.pi * (dow / 7.0)\n",
    "\n",
    "    sin_hour = np.sin(hour_rad)\n",
    "    cos_hour = np.cos(hour_rad)\n",
    "    sin_dow = np.sin(dow_rad)\n",
    "    cos_dow = np.cos(dow_rad)\n",
    "\n",
    "    is_weekend = ((dow >= 5).astype(float))\n",
    "    # Simple rush-hour proxy (tune if needed): 7–10 and 16–19\n",
    "    is_rush = (((hour >= 7) & (hour <= 10)) | ((hour >= 16) & (hour <= 19))).astype(float)\n",
    "\n",
    "    return np.stack([sin_hour, cos_hour, sin_dow, cos_dow, is_weekend, is_rush], axis=1)\n",
    "\n",
    "X_time = data_interface.build_time_features(meta[\"timestamps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f861f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def locf_impute_baseline(x_t, start_idx, end_idx, detector_idx):\n",
    "    \"\"\"\n",
    "    Naive 'last observation carried forward' baseline for imputation.\n",
    "    Uses the last finite value before blackout; falls back to\n",
    "    detector-wise historical mean if needed.\n",
    "    \"\"\"\n",
    "    last_idx = start_idx - 1\n",
    "    while last_idx >= 0 and not np.isfinite(x_t[last_idx, detector_idx]):\n",
    "        last_idx -= 1\n",
    "\n",
    "    if last_idx < 0:\n",
    "        det_vals = x_t[:, detector_idx]\n",
    "        last_val = float(np.nanmean(det_vals))\n",
    "    else:\n",
    "        last_val = float(x_t[last_idx, detector_idx])\n",
    "\n",
    "    length = end_idx - start_idx + 1\n",
    "    return np.full(length, last_val, dtype=float)\n",
    "\n",
    "\n",
    "def locf_forecast_baseline(x_t, end_idx, detector_idx):\n",
    "    \"\"\"\n",
    "    Naive baseline for forecasting: hold the last available observation\n",
    "    at the end of the blackout.\n",
    "    \"\"\"\n",
    "    last_idx = end_idx\n",
    "    while last_idx >= 0 and not np.isfinite(x_t[last_idx, detector_idx]):\n",
    "        last_idx -= 1\n",
    "\n",
    "    if last_idx < 0:\n",
    "        det_vals = x_t[:, detector_idx]\n",
    "        last_val = float(np.nanmean(det_vals))\n",
    "    else:\n",
    "        last_val = float(x_t[last_idx, detector_idx])\n",
    "\n",
    "    return last_val\n",
    "\n",
    "\n",
    "def evaluate_impute_forecast_model(\n",
    "    model,\n",
    "    mu_smooth,\n",
    "    Sigma_smooth,\n",
    "    mu_filt,\n",
    "    Sigma_filt,\n",
    "    x_t,\n",
    "    meta,\n",
    "    label=\"model\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Re-usable evaluation for any LDS-like model (MAR or MNAR):\n",
    "    - blackout imputation RMSE/MAE (length-weighted)\n",
    "    - 1 / 3 / 6-step forecast RMSE/MAE\n",
    "    \"\"\"\n",
    "    # ---------------- Imputation ----------------\n",
    "    impute_mae_list = []\n",
    "    impute_mse_list = []\n",
    "\n",
    "    for window in impute_evaluation_windows_val:\n",
    "        if window[\"test_type\"] != \"impute\":\n",
    "            continue\n",
    "\n",
    "        start_idx = np.where(meta[\"timestamps\"] == window[\"blackout_start\"])[0][0]\n",
    "        end_idx = np.where(meta[\"timestamps\"] == window[\"blackout_end\"])[0][0]\n",
    "        detector_idx = np.where(meta[\"detectors\"] == window[\"detector_id\"])[0][0]\n",
    "\n",
    "        eval_x_t = x_t[start_idx : end_idx + 1].copy()\n",
    "        eval_mu_smooth = mu_smooth[start_idx : end_idx + 1]\n",
    "        eval_Sigma_smooth = Sigma_smooth[start_idx : end_idx + 1]\n",
    "\n",
    "        reconstruct_x_t, _ = model.reconstruct_from_smoother(\n",
    "            eval_mu_smooth, eval_Sigma_smooth\n",
    "        )\n",
    "\n",
    "        y_true = eval_x_t[:, detector_idx]\n",
    "        y_pred = reconstruct_x_t[:, detector_idx]\n",
    "\n",
    "        mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        mae = sklearn.metrics.mean_absolute_error(y_pred[mask], y_true[mask])\n",
    "        mse = sklearn.metrics.mean_squared_error(y_pred[mask], y_true[mask])\n",
    "\n",
    "        impute_mae_list.append([mae, window[\"len_steps\"]])\n",
    "        impute_mse_list.append([mse, window[\"len_steps\"]])\n",
    "\n",
    "    final_mae = np.average(\n",
    "        [item[0] for item in impute_mae_list],\n",
    "        weights=[item[1] for item in impute_mae_list],\n",
    "    )\n",
    "    final_mse = np.average(\n",
    "        [item[0] for item in impute_mse_list],\n",
    "        weights=[item[1] for item in impute_mse_list],\n",
    "    )\n",
    "    final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "    print(f\"\\n[{label}] Imputation performance:\")\n",
    "    print(\"  MAE :\", final_mae)\n",
    "    print(\"  MSE :\", final_mse)\n",
    "    print(\"  RMSE:\", final_rmse)\n",
    "\n",
    "    # ---------------- Forecasting ----------------\n",
    "    y_actual_1_step, y_forecast_1_step = [], []\n",
    "    y_actual_3_step, y_forecast_3_step = [], []\n",
    "    y_actual_6_step, y_forecast_6_step = [], []\n",
    "\n",
    "    forecast_evaluation_windows_val = (\n",
    "        forecast_1_evaluation_windows_val\n",
    "        + forecast_3_evaluation_windows_val\n",
    "        + forecast_6_evaluation_windows_val\n",
    "    )\n",
    "\n",
    "    for window in forecast_evaluation_windows_val:\n",
    "        if window[\"test_type\"] != \"forecast\":\n",
    "            continue\n",
    "\n",
    "        start_idx = np.where(meta[\"timestamps\"] == window[\"blackout_start\"])[0][0]\n",
    "        end_idx = np.where(meta[\"timestamps\"] == window[\"blackout_end\"])[0][0]\n",
    "        detector_idx = np.where(meta[\"detectors\"] == window[\"detector_id\"])[0][0]\n",
    "        horizon = int(window[\"horizon_steps\"])\n",
    "\n",
    "        # Skip windows too close to the end of the series\n",
    "        if end_idx + horizon >= x_t.shape[0]:\n",
    "            continue\n",
    "\n",
    "        eval_x_t = x_t[end_idx + 1 : end_idx + 1 + horizon].copy()\n",
    "\n",
    "        forecast_x_t, _ = model.k_step_forecast(\n",
    "            mu_filt, Sigma_filt, end_idx, k=horizon\n",
    "        )\n",
    "\n",
    "        y_true = eval_x_t[horizon - 1, detector_idx]\n",
    "        y_pred = forecast_x_t[detector_idx]\n",
    "\n",
    "        if not (np.isfinite(y_true) and np.isfinite(y_pred)):\n",
    "            continue\n",
    "\n",
    "        if horizon == 1:\n",
    "            y_forecast_1_step.append(y_pred)\n",
    "            y_actual_1_step.append(y_true)\n",
    "        elif horizon == 3:\n",
    "            y_forecast_3_step.append(y_pred)\n",
    "            y_actual_3_step.append(y_true)\n",
    "        elif horizon == 6:\n",
    "            y_forecast_6_step.append(y_pred)\n",
    "            y_actual_6_step.append(y_true)\n",
    "\n",
    "    mae_1_step = sklearn.metrics.mean_absolute_error(\n",
    "        y_forecast_1_step, y_actual_1_step\n",
    "    )\n",
    "    mse_1_step = sklearn.metrics.mean_squared_error(\n",
    "        y_forecast_1_step, y_actual_1_step\n",
    "    )\n",
    "    rmse_1_step = np.sqrt(mse_1_step)\n",
    "\n",
    "    mae_3_step = sklearn.metrics.mean_absolute_error(\n",
    "        y_forecast_3_step, y_actual_3_step\n",
    "    )\n",
    "    mse_3_step = sklearn.metrics.mean_squared_error(\n",
    "        y_forecast_3_step, y_actual_3_step\n",
    "    )\n",
    "    rmse_3_step = np.sqrt(mse_3_step)\n",
    "\n",
    "    mae_6_step = sklearn.metrics.mean_absolute_error(\n",
    "        y_forecast_6_step, y_actual_6_step\n",
    "    )\n",
    "    mse_6_step = sklearn.metrics.mean_squared_error(\n",
    "        y_forecast_6_step, y_actual_6_step\n",
    "    )\n",
    "    rmse_6_step = np.sqrt(mse_6_step)\n",
    "\n",
    "    print(f\"\\n[{label}] Forecasting performance:\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"1-step MAE :\", mae_1_step)\n",
    "    print(\"1-step MSE :\", mse_1_step)\n",
    "    print(\"1-step RMSE:\", rmse_1_step)\n",
    "\n",
    "    print(\"\\n-----------------------------------\")\n",
    "    print(\"3-step MAE :\", mae_3_step)\n",
    "    print(\"3-step MSE :\", mse_3_step)\n",
    "    print(\"3-step RMSE:\", rmse_3_step)\n",
    "\n",
    "    print(\"\\n-----------------------------------\")\n",
    "    print(\"6-step MAE :\", mae_6_step)\n",
    "    print(\"6-step MSE :\", mse_6_step)\n",
    "    print(\"6-step RMSE:\", rmse_6_step)\n",
    "\n",
    "    return {\n",
    "        \"impute_mae\": final_mae,\n",
    "        \"impute_mse\": final_mse,\n",
    "        \"impute_rmse\": final_rmse,\n",
    "        \"forecast_mae_1\": mae_1_step,\n",
    "        \"forecast_mse_1\": mse_1_step,\n",
    "        \"forecast_rmse_1\": rmse_1_step,\n",
    "        \"forecast_mae_3\": mae_3_step,\n",
    "        \"forecast_mse_3\": mse_3_step,\n",
    "        \"forecast_rmse_3\": rmse_3_step,\n",
    "        \"forecast_mae_6\": mae_6_step,\n",
    "        \"forecast_mse_6\": mse_6_step,\n",
    "        \"forecast_rmse_6\": rmse_6_step,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0aa8f30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1) Stronger baselines\n",
    "#    - Linear interpolation inside blackout (uses pre + post points)\n",
    "#    - Optional spline interpolation (falls back to linear)\n",
    "#    - Seasonal naive for forecasting (same time yesterday / last week)\n",
    "# ============================================================\n",
    "def _find_last_finite(x: np.ndarray, idx: int, d: int) -> tuple[int, float] | tuple[None, None]:\n",
    "    j = idx\n",
    "    while j >= 0 and not np.isfinite(x[j, d]):\n",
    "        j -= 1\n",
    "    if j < 0:\n",
    "        return None, None\n",
    "    return j, float(x[j, d])\n",
    "\n",
    "\n",
    "def _find_next_finite(x: np.ndarray, idx: int, d: int) -> tuple[int, float] | tuple[None, None]:\n",
    "    j = idx\n",
    "    T = x.shape[0]\n",
    "    while j < T and not np.isfinite(x[j, d]):\n",
    "        j += 1\n",
    "    if j >= T:\n",
    "        return None, None\n",
    "    return j, float(x[j, d])\n",
    "\n",
    "\n",
    "def linear_interp_impute_baseline(x_t_masked, start_idx, end_idx, detector_idx):\n",
    "    \"\"\"\n",
    "    Impute a blackout [start_idx, end_idx] by linear interpolation between:\n",
    "      left  = last finite before start\n",
    "      right = first finite after end\n",
    "    Falls back to LOCF if one side missing.\n",
    "    \"\"\"\n",
    "    left_j, left_v = _find_last_finite(x_t_masked, start_idx - 1, detector_idx)\n",
    "    right_j, right_v = _find_next_finite(x_t_masked, end_idx + 1, detector_idx)\n",
    "\n",
    "    L = end_idx - start_idx + 1\n",
    "    if left_v is None and right_v is None:\n",
    "        # ultimate fallback: detector mean\n",
    "        det_vals = x_t_masked[:, detector_idx]\n",
    "        fill = float(np.nanmean(det_vals))\n",
    "        return np.full(L, fill, dtype=float)\n",
    "    if left_v is None:\n",
    "        return np.full(L, right_v, dtype=float)\n",
    "    if right_v is None:\n",
    "        return np.full(L, left_v, dtype=float)\n",
    "\n",
    "    # interpolate over actual index distance so long gaps are handled correctly\n",
    "    xs = np.arange(start_idx, end_idx + 1)\n",
    "    denom = max((right_j - left_j), 1)\n",
    "    alpha = (xs - left_j) / denom\n",
    "    return (1 - alpha) * left_v + alpha * right_v\n",
    "\n",
    "\n",
    "def spline_impute_baseline(x_t_masked, start_idx, end_idx, detector_idx, order=3):\n",
    "    \"\"\"\n",
    "    Optional spline interpolation via pandas (requires scipy).\n",
    "    Falls back to linear interpolation if spline unavailable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = pd.Series(x_t_masked[:, detector_idx]).astype(float)\n",
    "        # only interpolate the blackout segment; uses surrounding points\n",
    "        s2 = s.copy()\n",
    "        s2.iloc[start_idx:end_idx+1] = np.nan\n",
    "        s2 = s2.interpolate(method=\"spline\", order=order, limit_direction=\"both\")\n",
    "        return s2.iloc[start_idx:end_idx+1].to_numpy(dtype=float)\n",
    "    except Exception:\n",
    "        return linear_interp_impute_baseline(x_t_masked, start_idx, end_idx, detector_idx)\n",
    "\n",
    "\n",
    "def seasonal_naive_forecast_baseline(x_t_masked, target_idx, detector_idx, offsets=(288, 2016)):\n",
    "    \"\"\"\n",
    "    Forecast x[target_idx, d] using historical seasonal offsets:\n",
    "      - 288 steps = 1 day back (5-min grid)\n",
    "      - 2016 steps = 1 week back\n",
    "    Falls back to LOCF at end of blackout if needed.\n",
    "    \"\"\"\n",
    "    for off in offsets:\n",
    "        j = target_idx - off\n",
    "        if j >= 0 and np.isfinite(x_t_masked[j, detector_idx]):\n",
    "            return float(x_t_masked[j, detector_idx])\n",
    "    # fallback: last observed before target\n",
    "    j, v = _find_last_finite(x_t_masked, target_idx - 1, detector_idx)\n",
    "    if v is None:\n",
    "        det_vals = x_t_masked[:, detector_idx]\n",
    "        return float(np.nanmean(det_vals))\n",
    "    return v\n",
    "\n",
    "def build_hour_of_week_climatology(\n",
    "    x_t_masked: np.ndarray,\n",
    "    m_t_masked: np.ndarray,\n",
    "    timestamps: np.ndarray,\n",
    "    step_minutes: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds time-of-week climatology: mean per (slot_of_week, detector).\n",
    "\n",
    "    slot_of_week = dow * steps_per_day + step_in_day\n",
    "    where step_in_day = (hour*60 + minute) // step_minutes\n",
    "\n",
    "    Returns:\n",
    "      how_mean: (S, D) array of means, S = 7 * steps_per_day\n",
    "      slot_of_week: (T,) int array mapping each t -> slot index\n",
    "      global_mean: (D,) detector-wise global mean fallback\n",
    "    \"\"\"\n",
    "    ts = pd.to_datetime(timestamps)\n",
    "    dow = ts.dayofweek.to_numpy()  # Mon=0\n",
    "    minutes = (ts.hour.to_numpy() * 60 + ts.minute.to_numpy())\n",
    "\n",
    "    steps_per_day = int((24 * 60) // step_minutes)\n",
    "    step_in_day = (minutes // step_minutes).astype(int)\n",
    "    # safety: if timestamps aren't aligned to step_minutes, clamp\n",
    "    step_in_day = np.clip(step_in_day, 0, steps_per_day - 1)\n",
    "\n",
    "    slot_of_week = (dow * steps_per_day + step_in_day).astype(int)\n",
    "    S = 7 * steps_per_day\n",
    "    T, D = x_t_masked.shape\n",
    "\n",
    "    # observed mask: 1 where observed & finite\n",
    "    obs = (m_t_masked == 0) & np.isfinite(x_t_masked)\n",
    "\n",
    "    # detector-wise global mean fallback (computed only from observed)\n",
    "    global_mean = np.nan_to_num(\n",
    "        (np.nansum(np.where(obs, x_t_masked, 0.0), axis=0) / (obs.sum(axis=0) + 1e-6)),\n",
    "        nan=0.0\n",
    "    ).astype(float)\n",
    "\n",
    "    sums = np.zeros((S, D), dtype=np.float64)\n",
    "    cnts = np.zeros((S, D), dtype=np.float64)\n",
    "\n",
    "    # single pass over time; vectorized over detectors\n",
    "    for t in range(T):\n",
    "        s = slot_of_week[t]\n",
    "        m = obs[t]  # (D,)\n",
    "        if not m.any():\n",
    "            continue\n",
    "        sums[s, m] += x_t_masked[t, m]\n",
    "        cnts[s, m] += 1.0\n",
    "\n",
    "    how_mean = sums / np.maximum(cnts, 1.0)\n",
    "    how_mean[cnts < 1.0] = np.nan  # mark unseen slots as nan\n",
    "    return how_mean.astype(float), slot_of_week, global_mean\n",
    "\n",
    "\n",
    "def make_hour_of_week_forecast_fn(how_mean, slot_of_week, global_mean):\n",
    "    \"\"\"\n",
    "    Returns a forecast_fn compatible with evaluate_impute_forecast_baseline:\n",
    "      forecast_fn(x_t_masked, target_idx, detector_idx) -> float\n",
    "    \"\"\"\n",
    "    def _fn(x_t_masked, target_idx, detector_idx):\n",
    "        s = int(slot_of_week[target_idx])\n",
    "        mu = how_mean[s, detector_idx]\n",
    "        if np.isfinite(mu):\n",
    "            return float(mu)\n",
    "        # fallback: detector global mean, then LOCF\n",
    "        gm = global_mean[detector_idx]\n",
    "        if np.isfinite(gm):\n",
    "            return float(gm)\n",
    "        j, v = _find_last_finite(x_t_masked, target_idx - 1, detector_idx)\n",
    "        if v is None:\n",
    "            return 0.0\n",
    "        return float(v)\n",
    "    return _fn\n",
    "\n",
    "\n",
    "def evaluate_impute_forecast_baseline(\n",
    "    x_t_true,\n",
    "    x_t_masked,\n",
    "    meta,\n",
    "    impute_fn,\n",
    "    forecast_fn,\n",
    "    label=\"baseline\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Generic evaluator for baselines.\n",
    "      impute_fn(x_t_masked, start_idx, end_idx, detector_idx) -> (L,)\n",
    "      forecast_fn(x_t_masked, target_idx, detector_idx) -> float\n",
    "    \"\"\"\n",
    "    # ---------- Imputation ----------\n",
    "    impute_mae_list = []\n",
    "    impute_mse_list = []\n",
    "\n",
    "    for window in impute_evaluation_windows_val:\n",
    "        start_idx = np.where(meta[\"timestamps\"] == window[\"blackout_start\"])[0][0]\n",
    "        end_idx   = np.where(meta[\"timestamps\"] == window[\"blackout_end\"])[0][0]\n",
    "        d         = np.where(meta[\"detectors\"] == window[\"detector_id\"])[0][0]\n",
    "\n",
    "        y_true = x_t_true[start_idx:end_idx+1, d].copy()\n",
    "        y_pred = impute_fn(x_t_masked, start_idx, end_idx, d)\n",
    "\n",
    "        mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        mae = sklearn.metrics.mean_absolute_error(y_pred[mask], y_true[mask])\n",
    "        mse = sklearn.metrics.mean_squared_error(y_pred[mask], y_true[mask])\n",
    "        impute_mae_list.append([mae, window[\"len_steps\"]])\n",
    "        impute_mse_list.append([mse, window[\"len_steps\"]])\n",
    "\n",
    "    final_mae = np.average([a for a,_ in impute_mae_list], weights=[w for _,w in impute_mae_list])\n",
    "    final_mse = np.average([a for a,_ in impute_mse_list], weights=[w for _,w in impute_mse_list])\n",
    "    final_rmse = float(np.sqrt(final_mse))\n",
    "\n",
    "    print(f\"\\n[{label}] Imputation performance:\")\n",
    "    print(\"  MAE :\", final_mae)\n",
    "    print(\"  MSE :\", final_mse)\n",
    "    print(\"  RMSE:\", final_rmse)\n",
    "\n",
    "    # ---------- Forecast ----------\n",
    "    y_true_1, y_pred_1 = [], []\n",
    "    y_true_3, y_pred_3 = [], []\n",
    "    y_true_6, y_pred_6 = [], []\n",
    "\n",
    "    forecast_windows = (\n",
    "        forecast_1_evaluation_windows_val\n",
    "        + forecast_3_evaluation_windows_val\n",
    "        + forecast_6_evaluation_windows_val\n",
    "    )\n",
    "\n",
    "    for window in forecast_windows:\n",
    "        end_idx = np.where(meta[\"timestamps\"] == window[\"blackout_end\"])[0][0]\n",
    "        d       = np.where(meta[\"detectors\"] == window[\"detector_id\"])[0][0]\n",
    "        h       = int(window[\"horizon_steps\"])\n",
    "        target_idx = end_idx + h\n",
    "        if target_idx >= x_t_true.shape[0]:\n",
    "            continue\n",
    "\n",
    "        yt = x_t_true[target_idx, d]\n",
    "        yp = forecast_fn(x_t_masked, target_idx, d)\n",
    "        if not (np.isfinite(yt) and np.isfinite(yp)):\n",
    "            continue\n",
    "\n",
    "        if h == 1:\n",
    "            y_true_1.append(yt); y_pred_1.append(yp)\n",
    "        elif h == 3:\n",
    "            y_true_3.append(yt); y_pred_3.append(yp)\n",
    "        elif h == 6:\n",
    "            y_true_6.append(yt); y_pred_6.append(yp)\n",
    "\n",
    "    def _pack(y_pred, y_true, h):\n",
    "        mae = sklearn.metrics.mean_absolute_error(y_pred, y_true)\n",
    "        mse = sklearn.metrics.mean_squared_error(y_pred, y_true)\n",
    "        rmse = float(np.sqrt(mse))\n",
    "        print(f\"{h}-step MAE :\", mae)\n",
    "        print(f\"{h}-step MSE :\", mse)\n",
    "        print(f\"{h}-step RMSE:\", rmse)\n",
    "        return mae, mse, rmse\n",
    "\n",
    "    print(f\"\\n[{label}] Forecasting performance:\")\n",
    "    print(\"-----------------------------------\")\n",
    "    mae1,mse1,rmse1 = _pack(y_pred_1, y_true_1, 1)\n",
    "    print(\"\\n-----------------------------------\")\n",
    "    mae3,mse3,rmse3 = _pack(y_pred_3, y_true_3, 3)\n",
    "    print(\"\\n-----------------------------------\")\n",
    "    mae6,mse6,rmse6 = _pack(y_pred_6, y_true_6, 6)\n",
    "\n",
    "    return {\n",
    "        \"impute_mae\": final_mae,\n",
    "        \"impute_mse\": final_mse,\n",
    "        \"impute_rmse\": final_rmse,\n",
    "        \"forecast_mae_1\": mae1, \"forecast_mse_1\": mse1, \"forecast_rmse_1\": rmse1,\n",
    "        \"forecast_mae_3\": mae3, \"forecast_mse_3\": mse3, \"forecast_rmse_3\": rmse3,\n",
    "        \"forecast_mae_6\": mae6, \"forecast_mse_6\": mse6, \"forecast_rmse_6\": rmse6,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72ec092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_locf_baseline(\n",
    "    x_t_true,\n",
    "    x_t_masked,\n",
    "    meta,\n",
    "    label=\"LOCF baseline\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Baseline evaluation using LOCF for both imputation and forecasting.\n",
    "    x_t_true   : full panel (no artificial masking), used ONLY for y_true\n",
    "    x_t_masked : panel with blackout windows masked (same as training),\n",
    "                 used for baseline predictions so it can't peek inside.\n",
    "    \"\"\"\n",
    "    # ---------- Imputation ----------\n",
    "    impute_mae_list = []\n",
    "    impute_mse_list = []\n",
    "\n",
    "    for window in impute_evaluation_windows_val:\n",
    "        if window[\"test_type\"] != \"impute\":\n",
    "            continue\n",
    "\n",
    "        start_idx = np.where(meta[\"timestamps\"] == window[\"blackout_start\"])[0][0]\n",
    "        end_idx   = np.where(meta[\"timestamps\"] == window[\"blackout_end\"])[0][0]\n",
    "        detector_idx = np.where(meta[\"detectors\"] == window[\"detector_id\"])[0][0]\n",
    "\n",
    "        # Truth from full data\n",
    "        y_true = x_t_true[start_idx : end_idx + 1, detector_idx].copy()\n",
    "\n",
    "        # LOCF baseline only sees masked training panel\n",
    "        y_pred = locf_impute_baseline(\n",
    "            x_t_masked, start_idx, end_idx, detector_idx\n",
    "        )\n",
    "\n",
    "        mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        mae = sklearn.metrics.mean_absolute_error(y_pred[mask], y_true[mask])\n",
    "        mse = sklearn.metrics.mean_squared_error(y_pred[mask], y_true[mask])\n",
    "\n",
    "        impute_mae_list.append([mae, window[\"len_steps\"]])\n",
    "        impute_mse_list.append([mse, window[\"len_steps\"]])\n",
    "\n",
    "    final_mae = np.average(\n",
    "        [item[0] for item in impute_mae_list],\n",
    "        weights=[item[1] for item in impute_mae_list],\n",
    "    )\n",
    "    final_mse = np.average(\n",
    "        [item[0] for item in impute_mse_list],\n",
    "        weights=[item[1] for item in impute_mse_list],\n",
    "    )\n",
    "    final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "    print(f\"\\n[{label}] Imputation performance:\")\n",
    "    print(\"  MAE :\", final_mae)\n",
    "    print(\"  MSE :\", final_mse)\n",
    "    print(\"  RMSE:\", final_rmse)\n",
    "\n",
    "    # ---------- Forecast ----------\n",
    "    y_actual_1_step, y_forecast_1_step = [], []\n",
    "    y_actual_3_step, y_forecast_3_step = [], []\n",
    "    y_actual_6_step, y_forecast_6_step = [], []\n",
    "\n",
    "    forecast_evaluation_windows_val = (\n",
    "        forecast_1_evaluation_windows_val\n",
    "        + forecast_3_evaluation_windows_val\n",
    "        + forecast_6_evaluation_windows_val\n",
    "    )\n",
    "\n",
    "    for window in forecast_evaluation_windows_val:\n",
    "        if window[\"test_type\"] != \"forecast\":\n",
    "            continue\n",
    "\n",
    "        start_idx = np.where(meta[\"timestamps\"] == window[\"blackout_start\"])[0][0]\n",
    "        end_idx   = np.where(meta[\"timestamps\"] == window[\"blackout_end\"])[0][0]\n",
    "        detector_idx = np.where(meta[\"detectors\"] == window[\"detector_id\"])[0][0]\n",
    "        horizon = int(window[\"horizon_steps\"])\n",
    "\n",
    "        if end_idx + horizon >= x_t_true.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Truth from full data\n",
    "        y_true = x_t_true[end_idx + horizon, detector_idx]\n",
    "\n",
    "        # Baseline sees ONLY masked panel (so it uses last observed pre-blackout)\n",
    "        y_pred = locf_forecast_baseline(\n",
    "            x_t_masked, end_idx, detector_idx\n",
    "        )\n",
    "\n",
    "        if not (np.isfinite(y_true) and np.isfinite(y_pred)):\n",
    "            continue\n",
    "\n",
    "        if horizon == 1:\n",
    "            y_forecast_1_step.append(y_pred)\n",
    "            y_actual_1_step.append(y_true)\n",
    "        elif horizon == 3:\n",
    "            y_forecast_3_step.append(y_pred)\n",
    "            y_actual_3_step.append(y_true)\n",
    "        elif horizon == 6:\n",
    "            y_forecast_6_step.append(y_pred)\n",
    "            y_actual_6_step.append(y_true)\n",
    "\n",
    "    mae_1_step = sklearn.metrics.mean_absolute_error(\n",
    "        y_forecast_1_step, y_actual_1_step\n",
    "    )\n",
    "    mse_1_step = sklearn.metrics.mean_squared_error(\n",
    "        y_forecast_1_step, y_actual_1_step\n",
    "    )\n",
    "    rmse_1_step = np.sqrt(mse_1_step)\n",
    "\n",
    "    mae_3_step = sklearn.metrics.mean_absolute_error(\n",
    "        y_forecast_3_step, y_actual_3_step\n",
    "    )\n",
    "    mse_3_step = sklearn.metrics.mean_squared_error(\n",
    "        y_forecast_3_step, y_actual_3_step\n",
    "    )\n",
    "    rmse_3_step = np.sqrt(mse_3_step)\n",
    "\n",
    "    mae_6_step = sklearn.metrics.mean_absolute_error(\n",
    "        y_forecast_6_step, y_actual_6_step\n",
    "    )\n",
    "    mse_6_step = sklearn.metrics.mean_squared_error(\n",
    "        y_forecast_6_step, y_actual_6_step\n",
    "    )\n",
    "    rmse_6_step = np.sqrt(mse_6_step)\n",
    "\n",
    "    print(f\"\\n[{label}] Forecasting performance:\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"1-step MAE :\", mae_1_step)\n",
    "    print(\"1-step MSE :\", mse_1_step)\n",
    "    print(\"1-step RMSE:\", rmse_1_step)\n",
    "\n",
    "    print(\"\\n-----------------------------------\")\n",
    "    print(\"3-step MAE :\", mae_3_step)\n",
    "    print(\"3-step MSE :\", mse_3_step)\n",
    "    print(\"3-step RMSE:\", rmse_3_step)\n",
    "\n",
    "    print(\"\\n-----------------------------------\")\n",
    "    print(\"6-step MAE :\", mae_6_step)\n",
    "    print(\"6-step MSE :\", mse_6_step)\n",
    "    print(\"6-step RMSE:\", rmse_6_step)\n",
    "\n",
    "    return {\n",
    "        \"impute_mae\": final_mae,\n",
    "        \"impute_mse\": final_mse,\n",
    "        \"impute_rmse\": final_rmse,\n",
    "        \"forecast_mae_1\": mae_1_step,\n",
    "        \"forecast_mse_1\": mse_1_step,\n",
    "        \"forecast_rmse_1\": rmse_1_step,\n",
    "        \"forecast_mae_3\": mae_3_step,\n",
    "        \"forecast_mse_3\": mse_3_step,\n",
    "        \"forecast_rmse_3\": rmse_3_step,\n",
    "        \"forecast_mae_6\": mae_6_step,\n",
    "        \"forecast_mse_6\": mse_6_step,\n",
    "        \"forecast_rmse_6\": rmse_6_step,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33ee64d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_evaluation_windows(x_t, m_t, evaluation_windows_val, meta):\n",
    "    x_t_masked = x_t.copy()\n",
    "    m_t_masked = m_t.copy()\n",
    "\n",
    "    # Deduplicate (detector, start, end) so we don't re-mask the same block many times\n",
    "    unique_blocks = set()\n",
    "    for window in evaluation_windows_val:\n",
    "        start_ts = window[\"blackout_start\"]\n",
    "        end_ts = window[\"blackout_end\"]\n",
    "        det_id = window[\"detector_id\"]\n",
    "        unique_blocks.add((start_ts, end_ts, det_id))\n",
    "\n",
    "    for (start_ts, end_ts, det_id) in unique_blocks:\n",
    "        start_idx = np.where(meta[\"timestamps\"] == start_ts)[0][0]\n",
    "        end_idx = np.where(meta[\"timestamps\"] == end_ts)[0][0]\n",
    "        detector_idx = np.where(meta[\"detectors\"] == det_id)[0][0]\n",
    "\n",
    "        x_t_masked[start_idx:end_idx+1, detector_idx] = np.nan\n",
    "        m_t_masked[start_idx:end_idx+1, detector_idx] = 1\n",
    "\n",
    "    return x_t_masked, m_t_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4240d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data by masking evaluation windows\n",
    "x_t_train, m_t_train = mask_evaluation_windows(x_t, m_t, evaluation_windows_val, meta)\n",
    "latent_dim = 20\n",
    "D = x_t_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c48fe311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EM iteration 1/12 ===\n",
      "  A norm: 4.344\n",
      "  Q trace: 8.194\n",
      "  mean diag(R): 37.230\n",
      "\n",
      "=== EM iteration 2/12 ===\n",
      "  A norm: 4.225\n",
      "  Q trace: 24.719\n",
      "  mean diag(R): 22.689\n",
      "  max relative param change: 2.570e-01\n",
      "\n",
      "=== EM iteration 3/12 ===\n",
      "  A norm: 4.222\n",
      "  Q trace: 35.362\n",
      "  mean diag(R): 20.612\n",
      "  max relative param change: 1.206e-01\n",
      "\n",
      "=== EM iteration 4/12 ===\n",
      "  A norm: 4.215\n",
      "  Q trace: 39.576\n",
      "  mean diag(R): 20.497\n",
      "  max relative param change: 8.958e-02\n",
      "\n",
      "=== EM iteration 5/12 ===\n",
      "  A norm: 4.210\n",
      "  Q trace: 41.870\n",
      "  mean diag(R): 21.159\n",
      "  max relative param change: 8.940e-02\n",
      "\n",
      "=== EM iteration 6/12 ===\n",
      "  A norm: 4.192\n",
      "  Q trace: 46.512\n",
      "  mean diag(R): 22.482\n",
      "  max relative param change: 1.043e-01\n",
      "\n",
      "=== EM iteration 7/12 ===\n",
      "  A norm: 4.142\n",
      "  Q trace: 50.419\n",
      "  mean diag(R): 23.147\n",
      "  max relative param change: 9.172e-02\n",
      "\n",
      "=== EM iteration 8/12 ===\n",
      "  A norm: 4.094\n",
      "  Q trace: 53.918\n",
      "  mean diag(R): 23.430\n",
      "  max relative param change: 8.581e-02\n",
      "\n",
      "=== EM iteration 9/12 ===\n",
      "  A norm: 4.062\n",
      "  Q trace: 57.459\n",
      "  mean diag(R): 23.501\n",
      "  max relative param change: 7.608e-02\n",
      "\n",
      "=== EM iteration 10/12 ===\n",
      "  A norm: 4.032\n",
      "  Q trace: 59.619\n",
      "  mean diag(R): 23.586\n",
      "  max relative param change: 7.309e-02\n",
      "\n",
      "=== EM iteration 11/12 ===\n",
      "  A norm: 4.001\n",
      "  Q trace: 60.000\n",
      "  mean diag(R): 23.618\n",
      "  max relative param change: 6.973e-02\n",
      "\n",
      "=== EM iteration 12/12 ===\n",
      "  A norm: 3.963\n",
      "  Q trace: 59.958\n",
      "  mean diag(R): 23.612\n",
      "  max relative param change: 6.657e-02\n",
      "\n",
      "=== EM iteration 1/12 ===\n",
      "  A norm: 4.049\n",
      "  Q trace: 60.000\n",
      "  mean diag(R): 20.519\n",
      "\n",
      "=== EM iteration 2/12 ===\n",
      "  A norm: 4.112\n",
      "  Q trace: 52.829\n",
      "  mean diag(R): 19.869\n",
      "  max relative param change: 6.526e-02\n",
      "\n",
      "=== EM iteration 3/12 ===\n",
      "  A norm: 4.084\n",
      "  Q trace: 37.287\n",
      "  mean diag(R): 19.935\n",
      "  max relative param change: 6.054e-02\n",
      "\n",
      "=== EM iteration 4/12 ===\n",
      "  A norm: 4.045\n",
      "  Q trace: 25.206\n",
      "  mean diag(R): 20.238\n",
      "  max relative param change: 9.260e-02\n",
      "\n",
      "=== EM iteration 5/12 ===\n",
      "  A norm: 3.997\n",
      "  Q trace: 17.396\n",
      "  mean diag(R): 20.776\n",
      "  max relative param change: 1.342e-01\n",
      "\n",
      "=== EM iteration 6/12 ===\n",
      "  A norm: 3.966\n",
      "  Q trace: 12.978\n",
      "  mean diag(R): 21.532\n",
      "  max relative param change: 1.569e-01\n",
      "\n",
      "=== EM iteration 7/12 ===\n",
      "  A norm: 3.993\n",
      "  Q trace: 10.079\n",
      "  mean diag(R): 21.502\n",
      "  max relative param change: 1.469e-01\n",
      "\n",
      "=== EM iteration 8/12 ===\n",
      "  A norm: 4.031\n",
      "  Q trace: 8.018\n",
      "  mean diag(R): 21.317\n",
      "  max relative param change: 1.301e-01\n",
      "\n",
      "=== EM iteration 9/12 ===\n",
      "  A norm: 4.059\n",
      "  Q trace: 6.640\n",
      "  mean diag(R): 21.156\n",
      "  max relative param change: 1.092e-01\n",
      "\n",
      "=== EM iteration 10/12 ===\n",
      "  A norm: 4.081\n",
      "  Q trace: 5.733\n",
      "  mean diag(R): 21.006\n",
      "  max relative param change: 9.114e-02\n",
      "\n",
      "=== EM iteration 11/12 ===\n",
      "  A norm: 4.099\n",
      "  Q trace: 5.115\n",
      "  mean diag(R): 20.883\n",
      "  max relative param change: 7.699e-02\n",
      "\n",
      "=== EM iteration 12/12 ===\n",
      "  A norm: 4.111\n",
      "  Q trace: 4.680\n",
      "  mean diag(R): 20.791\n",
      "  max relative param change: 6.580e-02\n"
     ]
    }
   ],
   "source": [
    "from data_interface import build_time_features\n",
    "\n",
    "X_time = build_time_features(meta[\"timestamps\"])\n",
    "\n",
    "# ---------------- MAR model ----------------\n",
    "mar_params = mnar_blackout_lds.MNARParams.init_random(K=latent_dim, D=D, seed=42)\n",
    "model_mar = mnar_blackout_lds.MNARBlackoutLDS(mar_params)\n",
    "em_train_history_mar = model_mar.em_train(\n",
    "    x_t_train,\n",
    "    m_t_train,\n",
    "    X_time=X_time,\n",
    "    num_iters=12,\n",
    "    update_phi=False,\n",
    "    phi_steps=0,\n",
    "    phi_lr=0.0,\n",
    "    verbose=True,\n",
    "    convergence_tol=1e-3,\n",
    ")\n",
    "\n",
    "# ---------------- MNAR model ----------------\n",
    "mnar_params = copy.deepcopy(model_mar.params)\n",
    "\n",
    "# Initialize b from empirical missing rate\n",
    "eps = 1e-6\n",
    "p_miss_d = np.clip(m_t_train.mean(axis=0), eps, 1 - eps)\n",
    "mnar_params.b = np.log(p_miss_d / (1 - p_miss_d))\n",
    "\n",
    "model_mnar = mnar_blackout_lds.MNARBlackoutLDS(mnar_params)\n",
    "em_train_history_mnar = model_mnar.em_train(\n",
    "    x_t_train,\n",
    "    m_t_train,\n",
    "    X_time=X_time,\n",
    "    num_iters=12,\n",
    "    update_phi=True,\n",
    "    phi_steps=4,\n",
    "    phi_lr=5e-4,\n",
    "    verbose=True,\n",
    "    convergence_tol=1e-3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee32f79",
   "metadata": {},
   "source": [
    "### Reconstruction and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8c390c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MAR LDS] Imputation performance:\n",
      "  MAE : 5.421049097566691\n",
      "  MSE : 70.32399358378896\n",
      "  RMSE: 8.385940232543335\n",
      "\n",
      "[MAR LDS] Forecasting performance:\n",
      "-----------------------------------\n",
      "1-step MAE : 4.748208136663754\n",
      "1-step MSE : 53.04238739123073\n",
      "1-step RMSE: 7.283020485432588\n",
      "\n",
      "-----------------------------------\n",
      "3-step MAE : 5.104959797781098\n",
      "3-step MSE : 58.34928297969126\n",
      "3-step RMSE: 7.638670236349469\n",
      "\n",
      "-----------------------------------\n",
      "6-step MAE : 5.7794533988064165\n",
      "6-step MSE : 71.47613528938345\n",
      "6-step RMSE: 8.454355994952156\n"
     ]
    }
   ],
   "source": [
    "ekf_mar = model_mar.ekf_forward(x_t_train, m_t_train, X_time=X_time, use_missingness_obs=False)\n",
    "smoother_mar = model_mar.rts_smoother(ekf_mar)\n",
    "\n",
    "mu_filt_mar = ekf_mar[\"mu_filt\"]\n",
    "Sigma_filt_mar = ekf_mar[\"Sigma_filt\"]\n",
    "mu_smooth_mar = smoother_mar[\"mu_smooth\"]\n",
    "Sigma_smooth_mar = smoother_mar[\"Sigma_smooth\"]\n",
    "\n",
    "metrics_mar = evaluate_impute_forecast_model(\n",
    "    model=model_mar,\n",
    "    mu_smooth=mu_smooth_mar,\n",
    "    Sigma_smooth=Sigma_smooth_mar,\n",
    "    mu_filt=mu_filt_mar,\n",
    "    Sigma_filt=Sigma_filt_mar,\n",
    "    x_t=x_t,\n",
    "    meta=meta,\n",
    "    label=\"MAR LDS\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96f8e36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test1 (observed-only): onset vs control] AUC = 0.7374  (N=578, pos=0.516)\n",
      "[Test2 Balanced] N=60  pos_rate=0.250\n",
      "[OBS] ROC-AUC: 0.48214285714285715\n",
      "[OBS] PR-AUC : 0.29583333333333334\n",
      "[OBS] LogLoss: 0.8846304717482462\n",
      "[LAT] ROC-AUC: 0.5357142857142857\n",
      "[LAT] PR-AUC : 0.3138888888888889\n",
      "[LAT] LogLoss: 1.5093931476681492\n",
      "\n",
      "[Test3] Detector blackout durations (steps) summary:\n",
      "count      942.000000\n",
      "mean       849.284501\n",
      "std       7138.302338\n",
      "min         12.000000\n",
      "25%         18.000000\n",
      "50%         37.000000\n",
      "75%         84.000000\n",
      "max      88536.000000\n",
      "Name: len_steps, dtype: float64\n",
      "\n",
      "[Test3] Network blackout durations (steps) summary:\n",
      "count     25.00000\n",
      "mean      66.92000\n",
      "std       92.28214\n",
      "min        2.00000\n",
      "25%       19.00000\n",
      "50%       36.00000\n",
      "75%       84.00000\n",
      "max      427.00000\n",
      "Name: len_steps, dtype: float64\n",
      "\n",
      "[Test3] Network inter-event time (minutes) summary:\n",
      "count        24.000000\n",
      "mean      18596.041667\n",
      "std       28327.880873\n",
      "min          20.000000\n",
      "25%         748.750000\n",
      "50%        5830.000000\n",
      "75%       29030.000000\n",
      "max      115290.000000\n",
      "Name: start, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2) Missingness diagnosis (Tests 1–3)\n",
    "# ============================================================\n",
    "def build_blackout_onset_dataset(windows, x_t_true, m_t_true, meta, X_time, past_steps=12):\n",
    "    \"\"\"\n",
    "    Build (X, y) where each sample is a blackout ONSET edge:\n",
    "      time t0 = start_idx-1 (last observed before blackout)\n",
    "      label y = m[t0+1, d] (should be 1 for true onsets)\n",
    "    Features (observed-only):\n",
    "      - last observed speed at t0\n",
    "      - rolling variance over past_steps (default 1 hour = 12*5min)\n",
    "      - time features at t0\n",
    "    Returns:\n",
    "      X_obs: (N, 2+F_time)\n",
    "      y:     (N,)\n",
    "      t0_idx: (N,)\n",
    "      d_idx:  (N,)\n",
    "    \"\"\"\n",
    "    X_list, y_list, t0_list, d_list = [], [], [], []\n",
    "    for w in windows:\n",
    "        start_idx = np.where(meta[\"timestamps\"] == w[\"blackout_start\"])[0][0]\n",
    "        d = np.where(meta[\"detectors\"] == w[\"detector_id\"])[0][0]\n",
    "        t0 = start_idx - 1\n",
    "        if t0 <= past_steps or t0 + 1 >= x_t_true.shape[0]:\n",
    "            continue\n",
    "        last_speed = x_t_true[t0, d]\n",
    "        if not np.isfinite(last_speed):\n",
    "            continue\n",
    "        hist = x_t_true[t0-past_steps:t0, d]\n",
    "        hist = hist[np.isfinite(hist)]\n",
    "        if hist.size < max(3, past_steps//3):\n",
    "            continue\n",
    "        roll_var = float(np.var(hist))\n",
    "        feats = np.concatenate([[last_speed, roll_var], X_time[t0]], axis=0)\n",
    "        y = float(m_t_true[t0+1, d])  # whether next step is missing\n",
    "        X_list.append(feats); y_list.append(y); t0_list.append(t0); d_list.append(d)\n",
    "    return np.asarray(X_list, float), np.asarray(y_list, float), np.asarray(t0_list), np.asarray(d_list)\n",
    "\n",
    "\n",
    "def build_matched_control_dataset(N, x_t_true, m_t_true, X_time, t0_idx, d_idx, rng=42):\n",
    "    \"\"\"\n",
    "    Controls: sample (t,d) pairs with the SAME hour/weekend/rush distribution\n",
    "    as onsets by sampling t from the same t0_idx pool, but with random detectors,\n",
    "    and forcing label to be actual next-step missingness.\n",
    "    \"\"\"\n",
    "    rs = np.random.default_rng(rng)\n",
    "    Xc, yc = [], []\n",
    "    T, D = x_t_true.shape\n",
    "    for i in range(N):\n",
    "        t0 = int(t0_idx[rs.integers(0, len(t0_idx))])\n",
    "        d = int(rs.integers(0, D))\n",
    "        if t0 <= 12 or t0 + 1 >= T:\n",
    "            continue\n",
    "        last_speed = x_t_true[t0, d]\n",
    "        if not np.isfinite(last_speed):\n",
    "            continue\n",
    "        hist = x_t_true[t0-12:t0, d]\n",
    "        hist = hist[np.isfinite(hist)]\n",
    "        if hist.size < 4:\n",
    "            continue\n",
    "        roll_var = float(np.var(hist))\n",
    "        feats = np.concatenate([[last_speed, roll_var], X_time[t0]], axis=0)\n",
    "        y = float(m_t_true[t0+1, d])\n",
    "        Xc.append(feats); yc.append(y)\n",
    "    Xc = np.asarray(Xc, float)\n",
    "    yc = np.asarray(yc, float)\n",
    "    return Xc, yc\n",
    "\n",
    "\n",
    "def auc_logreg(X, y, label=\"clf\"):\n",
    "    clf = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "    clf.fit(X, y)\n",
    "    p = clf.predict_proba(X)[:, 1]\n",
    "    auc = roc_auc_score(y, p)\n",
    "    print(f\"[{label}] AUC = {auc:.4f}  (N={len(y)}, pos={y.mean():.3f})\")\n",
    "    return auc\n",
    "\n",
    "\n",
    "# --- Test 1: observed-only proxies near blackout edges ---\n",
    "X_on, y_on, t0_on, d_on = build_blackout_onset_dataset(\n",
    "    impute_evaluation_windows_val, x_t, m_t, meta, X_time, past_steps=12\n",
    ")\n",
    "X_ctrl, y_ctrl = build_matched_control_dataset(len(y_on), x_t, m_t, X_time, t0_on, d_on, rng=42)\n",
    "\n",
    "X1 = np.vstack([X_on, X_ctrl])\n",
    "y1 = np.concatenate([np.ones(len(y_on)), np.zeros(len(y_ctrl))])  # onset vs control\n",
    "auc_test1 = auc_logreg(X1, y1, label=\"Test1 (observed-only): onset vs control\")\n",
    "\n",
    "\n",
    "# --- Test 2: latent improves missingness prediction ---\n",
    "# Build two classifiers predicting next-step missingness:\n",
    "#   (a) time + last_speed\n",
    "#   (b) time + last_speed + smoothed latent state\n",
    "def build_nextstep_missingness_dataset_balanced(\n",
    "    x_t_true, m_t_true, X_time, mu_smooth,\n",
    "    sample_stride=12, max_pos=20_000, neg_per_pos=3, seed=0\n",
    "):\n",
    "    rs = np.random.default_rng(seed)\n",
    "    T, D = x_t_true.shape\n",
    "    pos, neg = [], []\n",
    "\n",
    "    for t in range(0, T - 2, sample_stride):\n",
    "        ds = rs.integers(0, D, size=min(64, D))\n",
    "        for d in ds:\n",
    "            if not np.isfinite(x_t_true[t, d]):\n",
    "                continue\n",
    "            y = int(m_t_true[t + 1, d])  # next-step missing\n",
    "\n",
    "            feat_obs = np.concatenate([[x_t_true[t, d]], X_time[t]])\n",
    "            feat_lat = np.concatenate([[x_t_true[t, d]], X_time[t], mu_smooth[t]])\n",
    "\n",
    "            if y == 1 and len(pos) < max_pos:\n",
    "                pos.append((t, feat_obs, feat_lat, 1))\n",
    "            elif y == 0 and (len(neg) < neg_per_pos * max(1, len(pos))):\n",
    "                neg.append((t, feat_obs, feat_lat, 0))\n",
    "\n",
    "    data = pos + neg\n",
    "    # IMPORTANT: keep time ordering for a time-aware split\n",
    "    data.sort(key=lambda z: z[0])  # sort by t\n",
    "\n",
    "    Xobs = np.asarray([a for _, a, _, _ in data], float)\n",
    "    Xlat = np.asarray([b for _, _, b, _ in data], float)\n",
    "    y    = np.asarray([c for *_, c in data], int)\n",
    "    return Xobs, Xlat, y\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Balanced Test2 (ROC-AUC + PR-AUC + LogLoss)\n",
    "# ----------------------------\n",
    "X2_obs, X2_lat, y2 = build_nextstep_missingness_dataset_balanced(\n",
    "    x_t_true=x_t,\n",
    "    m_t_true=m_t,\n",
    "    X_time=X_time,\n",
    "    mu_smooth=mu_smooth_mar,\n",
    "    sample_stride=12,\n",
    "    max_pos=20_000,\n",
    "    neg_per_pos=3,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "print(f\"[Test2 Balanced] N={len(y2)}  pos_rate={y2.mean():.3f}\")\n",
    "\n",
    "# Time-aware split: first 70% train, last 30% test (because we kept time order)\n",
    "n = len(y2)\n",
    "split = int(0.7 * n)\n",
    "\n",
    "Xobs_tr, Xobs_te = X2_obs[:split], X2_obs[split:]\n",
    "Xlat_tr, Xlat_te = X2_lat[:split], X2_lat[split:]\n",
    "y_tr, y_te       = y2[:split], y2[split:]\n",
    "\n",
    "clf_obs = LogisticRegression(max_iter=200, n_jobs=-1).fit(Xobs_tr, y_tr)\n",
    "clf_lat = LogisticRegression(max_iter=200, n_jobs=-1).fit(Xlat_tr, y_tr)\n",
    "\n",
    "for name, clf, Xte in [(\"OBS\", clf_obs, Xobs_te), (\"LAT\", clf_lat, Xlat_te)]:\n",
    "    p = clf.predict_proba(Xte)[:, 1]\n",
    "    # sklearn versions removed `eps=` in log_loss; clip manually instead\n",
    "    p = np.clip(p, 1e-7, 1 - 1e-7)\n",
    "    print(f\"[{name}] ROC-AUC:\", roc_auc_score(y_te, p))\n",
    "    print(f\"[{name}] PR-AUC :\", average_precision_score(y_te, p))\n",
    "    print(f\"[{name}] LogLoss:\", log_loss(y_te, p))\n",
    "\n",
    "\n",
    "# --- Test 3: event-level clustering / structure ---\n",
    "try:\n",
    "    det_events = data_interface.load_detector_blackouts(\"data\", as_dataframe=True)\n",
    "    net_events = data_interface.load_network_blackouts(\"data\", as_dataframe=True)\n",
    "\n",
    "    print(\"\\n[Test3] Detector blackout durations (steps) summary:\")\n",
    "    print(det_events[\"len_steps\"].describe())\n",
    "    print(\"\\n[Test3] Network blackout durations (steps) summary:\")\n",
    "    print(net_events[\"len_steps\"].describe())\n",
    "\n",
    "    # Simple clustering proxy: inter-event time (network-level)\n",
    "    net_starts = pd.to_datetime(net_events[\"start\"]).sort_values()\n",
    "    deltas_min = net_starts.diff().dropna().dt.total_seconds() / 60.0\n",
    "    print(\"\\n[Test3] Network inter-event time (minutes) summary:\")\n",
    "    print(deltas_min.describe())\n",
    "except Exception as e:\n",
    "    print(f\"[Test3] Skipped (missing blackout event parquet?): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ad9cd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MNAR LDS (report: const missingness var)] Imputation performance:\n",
      "  MAE : 5.330255647881937\n",
      "  MSE : 66.29155389355113\n",
      "  RMSE: 8.141962533293256\n",
      "\n",
      "[MNAR LDS (report: const missingness var)] Forecasting performance:\n",
      "-----------------------------------\n",
      "1-step MAE : 4.697742157492266\n",
      "1-step MSE : 50.374242781494495\n",
      "1-step RMSE: 7.097481439320183\n",
      "\n",
      "-----------------------------------\n",
      "3-step MAE : 5.0728911274653505\n",
      "3-step MSE : 56.15808247323508\n",
      "3-step RMSE: 7.493869659477344\n",
      "\n",
      "-----------------------------------\n",
      "6-step MAE : 5.699766120342447\n",
      "6-step MSE : 68.5219621844729\n",
      "6-step RMSE: 8.27779935637926\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Reconstruction & prediction: MNAR ----------------\n",
    "ekf_mnar = model_mnar.ekf_forward(\n",
    "    x_t_train,\n",
    "    m_t_train,\n",
    "    X_time=X_time,\n",
    "    use_missingness_obs=True,     \n",
    ")\n",
    "smoother_mnar = model_mnar.rts_smoother(ekf_mnar)\n",
    "\n",
    "mu_filt_mnar = ekf_mnar[\"mu_filt\"]\n",
    "Sigma_filt_mnar = ekf_mnar[\"Sigma_filt\"]\n",
    "mu_smooth_mnar = smoother_mnar[\"mu_smooth\"]\n",
    "Sigma_smooth_mnar = smoother_mnar[\"Sigma_smooth\"]\n",
    "\n",
    "metrics_mnar = evaluate_impute_forecast_model(\n",
    "    model=model_mnar,\n",
    "    mu_smooth=mu_smooth_mnar,\n",
    "    Sigma_smooth=Sigma_smooth_mnar,\n",
    "    mu_filt=mu_filt_mnar,\n",
    "    Sigma_filt=Sigma_filt_mnar,\n",
    "    x_t=x_t,\n",
    "    meta=meta,\n",
    "    label=\"MNAR LDS (report: const missingness var)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38c9e1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MNAR LDS (const missingness var)] Imputation performance:\n",
      "  MAE : 5.39648843115324\n",
      "  MSE : 67.61631888903317\n",
      "  RMSE: 8.22291425767247\n",
      "\n",
      "[MNAR LDS (const missingness var)] Forecasting performance:\n",
      "-----------------------------------\n",
      "1-step MAE : 4.757923626273731\n",
      "1-step MSE : 50.96568696378213\n",
      "1-step RMSE: 7.139025631259642\n",
      "\n",
      "-----------------------------------\n",
      "3-step MAE : 5.115397432570886\n",
      "3-step MSE : 56.20095663580049\n",
      "3-step RMSE: 7.496729729408717\n",
      "\n",
      "-----------------------------------\n",
      "6-step MAE : 5.7311422446607345\n",
      "6-step MSE : 68.25567849470387\n",
      "6-step RMSE: 8.261699491914715\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 3) Inference ablation: missingness variance\n",
    "#    (i) moment-matched (default) vs (ii) constant variance\n",
    "# ============================================================\n",
    "ekf_mnar_const = model_mnar.ekf_forward(\n",
    "    x_t_train, m_t_train,X_time=X_time,\n",
    "    missingness_var_mode=\"constant\",\n",
    ")\n",
    "smoother_mnar_const = model_mnar.rts_smoother(ekf_mnar_const)\n",
    "\n",
    "metrics_mnar_const = evaluate_impute_forecast_model(\n",
    "    model=model_mnar,\n",
    "    mu_smooth=smoother_mnar_const[\"mu_smooth\"],\n",
    "    Sigma_smooth=smoother_mnar_const[\"Sigma_smooth\"],\n",
    "    mu_filt=ekf_mnar_const[\"mu_filt\"],\n",
    "    Sigma_filt=ekf_mnar_const[\"Sigma_filt\"],\n",
    "    x_t=x_t,\n",
    "    meta=meta,\n",
    "    label=\"MNAR LDS (const missingness var)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f890c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LOCF baseline] Imputation performance:\n",
      "  MAE : 7.880999151920512\n",
      "  MSE : 181.6578376980285\n",
      "  RMSE: 13.478050218708509\n",
      "\n",
      "[LOCF baseline] Forecasting performance:\n",
      "-----------------------------------\n",
      "1-step MAE : 11.52644662019662\n",
      "1-step MSE : 318.6954144807004\n",
      "1-step RMSE: 17.852042305593507\n",
      "\n",
      "-----------------------------------\n",
      "3-step MAE : 12.502798017520238\n",
      "3-step MSE : 362.3945872475539\n",
      "3-step RMSE: 19.036664288880914\n",
      "\n",
      "-----------------------------------\n",
      "6-step MAE : 14.010510346065898\n",
      "6-step MSE : 407.7179884622047\n",
      "6-step RMSE: 20.192027844231117\n",
      "\n",
      "Done: LOCF vs MAR vs MNAR evaluated on the same blackout windows.\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Baseline: LOCF ----------------\n",
    "baseline_locf_metrics = evaluate_locf_baseline(\n",
    "    x_t_true=x_t,\n",
    "    x_t_masked=x_t_train,\n",
    "    meta=meta,\n",
    "    label=\"LOCF baseline\",\n",
    ")\n",
    "\n",
    "print(\"\\nDone: LOCF vs MAR vs MNAR evaluated on the same blackout windows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef08d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4) Bootstrap uncertainty on RMSE deltas (paper-ready)\n",
    "#    - window-resampling with replacement\n",
    "#    - length-weighted RMSE for imputation windows\n",
    "#    - plain RMSE for 1/3/6-step forecast windows\n",
    "# ============================================================\n",
    "def collect_impute_window_mse(model, mu_smooth, Sigma_smooth, x_t_true, meta):\n",
    "    \"\"\"\n",
    "    Returns lists aligned to impute_evaluation_windows_val:\n",
    "      mse_list: per-window mean squared error inside blackout (float)\n",
    "      w_list:   per-window weight = len_steps (int)\n",
    "    \"\"\"\n",
    "    mse_list, w_list = [], []\n",
    "    for w in impute_evaluation_windows_val:\n",
    "        start_idx = np.where(meta[\"timestamps\"] == w[\"blackout_start\"])[0][0]\n",
    "        end_idx   = np.where(meta[\"timestamps\"] == w[\"blackout_end\"])[0][0]\n",
    "        d         = np.where(meta[\"detectors\"] == w[\"detector_id\"])[0][0]\n",
    "\n",
    "        # reconstruct only within blackout slice\n",
    "        eval_mu = mu_smooth[start_idx:end_idx+1]\n",
    "        eval_S  = Sigma_smooth[start_idx:end_idx+1]\n",
    "        recon, _ = model.reconstruct_from_smoother(eval_mu, eval_S)\n",
    "\n",
    "        y_true = x_t_true[start_idx:end_idx+1, d]\n",
    "        y_pred = recon[:, d]\n",
    "        mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "        mse = float(np.mean((y_true[mask] - y_pred[mask])**2))\n",
    "        mse_list.append(mse)\n",
    "        w_list.append(int(w[\"len_steps\"]))\n",
    "    return np.asarray(mse_list, float), np.asarray(w_list, int)\n",
    "\n",
    "\n",
    "def collect_forecast_sqerr(model, mu_filt, Sigma_filt, x_t_true, meta, horizon: int):\n",
    "    \"\"\"\n",
    "    Returns per-window squared error list for given horizon.\n",
    "    Forecast is produced from end_idx using model.k_step_forecast (no peeking).\n",
    "    \"\"\"\n",
    "    sqerrs = []\n",
    "    windows = {1: forecast_1_evaluation_windows_val,\n",
    "               3: forecast_3_evaluation_windows_val,\n",
    "               6: forecast_6_evaluation_windows_val}[horizon]\n",
    "\n",
    "    for w in windows:\n",
    "        end_idx = np.where(meta[\"timestamps\"] == w[\"blackout_end\"])[0][0]\n",
    "        d       = np.where(meta[\"detectors\"] == w[\"detector_id\"])[0][0]\n",
    "        target_idx = end_idx + horizon\n",
    "        if target_idx >= x_t_true.shape[0]:\n",
    "            continue\n",
    "\n",
    "        y_true = float(x_t_true[target_idx, d])\n",
    "        if not np.isfinite(y_true):\n",
    "            continue\n",
    "\n",
    "        pred_x, _ = model.k_step_forecast(mu_filt, Sigma_filt, end_idx, k=horizon)\n",
    "        y_pred = float(pred_x[d])\n",
    "        if not np.isfinite(y_pred):\n",
    "            continue\n",
    "\n",
    "        sqerrs.append((y_true - y_pred)**2)\n",
    "\n",
    "    return np.asarray(sqerrs, float)\n",
    "\n",
    "\n",
    "def collect_forecast_sqerr_baseline(x_t_true, x_t_masked, meta, forecast_fn, horizon: int):\n",
    "    \"\"\"\n",
    "    Baseline version: forecast_fn(x_t_masked, target_idx, d) -> float\n",
    "    \"\"\"\n",
    "    sqerrs = []\n",
    "    windows = {1: forecast_1_evaluation_windows_val,\n",
    "               3: forecast_3_evaluation_windows_val,\n",
    "               6: forecast_6_evaluation_windows_val}[horizon]\n",
    "    for w in windows:\n",
    "        end_idx = np.where(meta[\"timestamps\"] == w[\"blackout_end\"])[0][0]\n",
    "        d       = np.where(meta[\"detectors\"] == w[\"detector_id\"])[0][0]\n",
    "        target_idx = end_idx + horizon\n",
    "        if target_idx >= x_t_true.shape[0]:\n",
    "            continue\n",
    "        y_true = float(x_t_true[target_idx, d])\n",
    "        y_pred = float(forecast_fn(x_t_masked, target_idx, d))\n",
    "        if not (np.isfinite(y_true) and np.isfinite(y_pred)):\n",
    "            continue\n",
    "        sqerrs.append((y_true - y_pred)**2)\n",
    "    return np.asarray(sqerrs, float)\n",
    "\n",
    "\n",
    "def _rmse_from_sqerrs(sqerrs: np.ndarray) -> float:\n",
    "    return float(np.sqrt(np.mean(sqerrs))) if sqerrs.size else np.nan\n",
    "\n",
    "\n",
    "def _weighted_rmse_from_window_mse(mse_list: np.ndarray, w_list: np.ndarray) -> float:\n",
    "    if mse_list.size == 0:\n",
    "        return np.nan\n",
    "    return float(np.sqrt(np.average(mse_list, weights=w_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca619031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bootstrap RMSE deltas (delta = A - B; negative means A better) ===\n",
      "Impute RMSE: MAR - LOCF = -5.0921  CI95=(-7.0998724022880175, -3.0292486885804264)\n",
      "Fcast1 RMSE: MAR - LOCF = -10.5690  CI95=(-12.646440837166292, -8.106431925662411)\n",
      "Fcast3 RMSE: MAR - LOCF = -11.3980  CI95=(-13.451453222550814, -9.145692835320263)\n",
      "Fcast6 RMSE: MAR - LOCF = -11.7377  CI95=(-13.701436252005843, -9.553280102131584)\n",
      "\n",
      "Impute RMSE: MNAR - MAR = -0.2440  CI95=(-1.5589055096183935, 1.2333407544169206)\n",
      "Fcast1 RMSE: MNAR - MAR = -0.1855  CI95=(-1.5828058910198368, 1.2839101823746895)\n",
      "Fcast3 RMSE: MNAR - MAR = -0.1448  CI95=(-1.4951432853203375, 1.0912836638542414)\n",
      "Fcast6 RMSE: MNAR - MAR = -0.1766  CI95=(-1.5759430465968478, 1.1617406523019946)\n",
      "\n",
      "Fcast1 RMSE: SeasonalNaive - HourOfWeek = +3.9765  CI95=(1.6405572258517576, 6.156798138636804)\n",
      "Fcast3 RMSE: SeasonalNaive - HourOfWeek = +2.5999  CI95=(0.346698429252156, 4.981561996180359)\n",
      "Fcast6 RMSE: SeasonalNaive - HourOfWeek = +3.0434  CI95=(0.7548304148588332, 5.1679912975858695)\n",
      "\n",
      "Fcast1 RMSE: MAR - HourOfWeek = -2.5402  CI95=(-4.0368974620071745, -0.8778159543675986)\n",
      "Fcast3 RMSE: MAR - HourOfWeek = -1.6510  CI95=(-3.1911947498148288, 0.10779924512613714)\n",
      "Fcast6 RMSE: MAR - HourOfWeek = -0.4270  CI95=(-1.8227859406582667, 1.0671943960474146)\n",
      "\n",
      "Fcast1 RMSE: MNAR - HourOfWeek = -2.7258  CI95=(-4.3602194393678575, -1.0796291419698647)\n",
      "Fcast3 RMSE: MNAR - HourOfWeek = -1.7958  CI95=(-3.6182760753424343, 0.04446511772184274)\n",
      "Fcast6 RMSE: MNAR - HourOfWeek = -0.6036  CI95=(-2.0747265904756786, 0.9400596400290873)\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_rmse_delta(\n",
    "    a_values: np.ndarray,\n",
    "    b_values: np.ndarray,\n",
    "    n_boot: int = 500,\n",
    "    seed: int = 0,\n",
    "    mode: str = \"sqerr\",         # \"sqerr\" or \"win_mse\"\n",
    "    a_weights: np.ndarray | None = None,\n",
    "    b_weights: np.ndarray | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Bootstraps delta = RMSE(a) - RMSE(b). Lower is better => negative delta is improvement.\n",
    "    mode:\n",
    "      - \"sqerr\": a_values/b_values are per-window squared errors, RMSE = sqrt(mean)\n",
    "      - \"win_mse\": a_values/b_values are per-window MSE, RMSE = sqrt(weighted avg MSE)\n",
    "    Returns: dict with point_est, CI, boot_samples\n",
    "    \"\"\"\n",
    "    rs = np.random.default_rng(seed)\n",
    "\n",
    "    # point estimates\n",
    "    if mode == \"sqerr\":\n",
    "        rmse_a = _rmse_from_sqerrs(a_values)\n",
    "        rmse_b = _rmse_from_sqerrs(b_values)\n",
    "    else:\n",
    "        rmse_a = _weighted_rmse_from_window_mse(a_values, a_weights)\n",
    "        rmse_b = _weighted_rmse_from_window_mse(b_values, b_weights)\n",
    "\n",
    "    point = rmse_a - rmse_b\n",
    "\n",
    "    # bootstrap\n",
    "    boots = []\n",
    "    n_a = len(a_values)\n",
    "    n_b = len(b_values)\n",
    "    for _ in range(n_boot):\n",
    "        ia = rs.integers(0, n_a, size=n_a)\n",
    "        ib = rs.integers(0, n_b, size=n_b)\n",
    "\n",
    "        if mode == \"sqerr\":\n",
    "            ra = _rmse_from_sqerrs(a_values[ia])\n",
    "            rb = _rmse_from_sqerrs(b_values[ib])\n",
    "        else:\n",
    "            ra = _weighted_rmse_from_window_mse(a_values[ia], a_weights[ia])\n",
    "            rb = _weighted_rmse_from_window_mse(b_values[ib], b_weights[ib])\n",
    "        boots.append(ra - rb)\n",
    "\n",
    "    boots = np.asarray(boots, float)\n",
    "    lo, hi = np.quantile(boots, [0.025, 0.975])\n",
    "    return {\n",
    "        \"point_delta\": float(point),\n",
    "        \"ci95\": (float(lo), float(hi)),\n",
    "        \"boot\": boots,\n",
    "        \"rmse_a\": float(rmse_a),\n",
    "        \"rmse_b\": float(rmse_b),\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------- Build error arrays ----------\n",
    "# Imputation (window MSE + len_steps weights)\n",
    "mar_imp_mse, mar_imp_w = collect_impute_window_mse(model_mar, mu_smooth_mar, Sigma_smooth_mar, x_t, meta)\n",
    "mnar_imp_mse, mnar_imp_w = collect_impute_window_mse(model_mnar, mu_smooth_mnar, Sigma_smooth_mnar, x_t, meta)\n",
    "\n",
    "# LOCF imputation: compute per-window MSE (still cheap)\n",
    "def collect_impute_window_mse_baseline(x_t_true, x_t_masked, meta, impute_fn):\n",
    "    mse_list, w_list = [], []\n",
    "    for w in impute_evaluation_windows_val:\n",
    "        start_idx = np.where(meta[\"timestamps\"] == w[\"blackout_start\"])[0][0]\n",
    "        end_idx   = np.where(meta[\"timestamps\"] == w[\"blackout_end\"])[0][0]\n",
    "        d         = np.where(meta[\"detectors\"] == w[\"detector_id\"])[0][0]\n",
    "        y_true = x_t_true[start_idx:end_idx+1, d]\n",
    "        y_pred = impute_fn(x_t_masked, start_idx, end_idx, d)\n",
    "        mask = np.isfinite(y_true) & np.isfinite(y_pred)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "        mse = float(np.mean((y_true[mask] - y_pred[mask])**2))\n",
    "        mse_list.append(mse)\n",
    "        w_list.append(int(w[\"len_steps\"]))\n",
    "    return np.asarray(mse_list, float), np.asarray(w_list, int)\n",
    "\n",
    "locf_imp_mse, locf_imp_w = collect_impute_window_mse_baseline(x_t, x_t_train, meta, locf_impute_baseline)\n",
    "\n",
    "# Forecast sqerr arrays per horizon\n",
    "mar_sq1 = collect_forecast_sqerr(model_mar, mu_filt_mar, Sigma_filt_mar, x_t, meta, horizon=1)\n",
    "mar_sq3 = collect_forecast_sqerr(model_mar, mu_filt_mar, Sigma_filt_mar, x_t, meta, horizon=3)\n",
    "mar_sq6 = collect_forecast_sqerr(model_mar, mu_filt_mar, Sigma_filt_mar, x_t, meta, horizon=6)\n",
    "\n",
    "mnar_sq1 = collect_forecast_sqerr(model_mnar, mu_filt_mnar, Sigma_filt_mnar, x_t, meta, horizon=1)\n",
    "mnar_sq3 = collect_forecast_sqerr(model_mnar, mu_filt_mnar, Sigma_filt_mnar, x_t, meta, horizon=3)\n",
    "mnar_sq6 = collect_forecast_sqerr(model_mnar, mu_filt_mnar, Sigma_filt_mnar, x_t, meta, horizon=6)\n",
    "\n",
    "locf_sq1 = collect_forecast_sqerr_baseline(x_t, x_t_train, meta, lambda xt, ti, d: locf_forecast_baseline(xt, ti-1, d), horizon=1)\n",
    "locf_sq3 = collect_forecast_sqerr_baseline(x_t, x_t_train, meta, lambda xt, ti, d: locf_forecast_baseline(xt, ti-3, d), horizon=3)\n",
    "locf_sq6 = collect_forecast_sqerr_baseline(x_t, x_t_train, meta, lambda xt, ti, d: locf_forecast_baseline(xt, ti-6, d), horizon=6)\n",
    "\n",
    "season_sq1 = collect_forecast_sqerr_baseline(x_t, x_t_train, meta, seasonal_naive_forecast_baseline, horizon=1)\n",
    "season_sq3 = collect_forecast_sqerr_baseline(x_t, x_t_train, meta, seasonal_naive_forecast_baseline, horizon=3)\n",
    "season_sq6 = collect_forecast_sqerr_baseline(x_t, x_t_train, meta, seasonal_naive_forecast_baseline, horizon=6)\n",
    "\n",
    "# ---------- Hour-of-Week baseline sqerr arrays ----------\n",
    "how_mean, slot_of_week, global_mean = build_hour_of_week_climatology(\n",
    "    x_t_masked=x_t_train,\n",
    "    m_t_masked=m_t_train,\n",
    "    timestamps=meta[\"timestamps\"],\n",
    "    step_minutes=5,\n",
    ")\n",
    "hour_of_week_forecast_fn = make_hour_of_week_forecast_fn(how_mean, slot_of_week, global_mean)\n",
    "\n",
    "how_sq1 = collect_forecast_sqerr_baseline(x_t, x_t_train, meta, hour_of_week_forecast_fn, horizon=1)\n",
    "how_sq3 = collect_forecast_sqerr_baseline(x_t, x_t_train, meta, hour_of_week_forecast_fn, horizon=3)\n",
    "how_sq6 = collect_forecast_sqerr_baseline(x_t, x_t_train, meta, hour_of_week_forecast_fn, horizon=6)\n",
    "\n",
    "# ---------- Bootstrap deltas ----------\n",
    "print(\"\\n=== Bootstrap RMSE deltas (delta = A - B; negative means A better) ===\")\n",
    "\n",
    "# MAR vs LOCF\n",
    "res_imp_mar_locf = bootstrap_rmse_delta(mar_imp_mse, locf_imp_mse, n_boot=500, seed=0, mode=\"win_mse\", a_weights=mar_imp_w, b_weights=locf_imp_w)\n",
    "print(f\"Impute RMSE: MAR - LOCF = {res_imp_mar_locf['point_delta']:+.4f}  CI95={res_imp_mar_locf['ci95']}\")\n",
    "\n",
    "res_f1_mar_locf = bootstrap_rmse_delta(mar_sq1, locf_sq1, n_boot=500, seed=1, mode=\"sqerr\")\n",
    "res_f3_mar_locf = bootstrap_rmse_delta(mar_sq3, locf_sq3, n_boot=500, seed=2, mode=\"sqerr\")\n",
    "res_f6_mar_locf = bootstrap_rmse_delta(mar_sq6, locf_sq6, n_boot=500, seed=3, mode=\"sqerr\")\n",
    "print(f\"Fcast1 RMSE: MAR - LOCF = {res_f1_mar_locf['point_delta']:+.4f}  CI95={res_f1_mar_locf['ci95']}\")\n",
    "print(f\"Fcast3 RMSE: MAR - LOCF = {res_f3_mar_locf['point_delta']:+.4f}  CI95={res_f3_mar_locf['ci95']}\")\n",
    "print(f\"Fcast6 RMSE: MAR - LOCF = {res_f6_mar_locf['point_delta']:+.4f}  CI95={res_f6_mar_locf['ci95']}\")\n",
    "\n",
    "# MNAR vs MAR\n",
    "res_imp_mnar_mar = bootstrap_rmse_delta(mnar_imp_mse, mar_imp_mse, n_boot=500, seed=10, mode=\"win_mse\", a_weights=mnar_imp_w, b_weights=mar_imp_w)\n",
    "print(f\"\\nImpute RMSE: MNAR - MAR = {res_imp_mnar_mar['point_delta']:+.4f}  CI95={res_imp_mnar_mar['ci95']}\")\n",
    "\n",
    "res_f1_mnar_mar = bootstrap_rmse_delta(mnar_sq1, mar_sq1, n_boot=500, seed=11, mode=\"sqerr\")\n",
    "res_f3_mnar_mar = bootstrap_rmse_delta(mnar_sq3, mar_sq3, n_boot=500, seed=12, mode=\"sqerr\")\n",
    "res_f6_mnar_mar = bootstrap_rmse_delta(mnar_sq6, mar_sq6, n_boot=500, seed=13, mode=\"sqerr\")\n",
    "print(f\"Fcast1 RMSE: MNAR - MAR = {res_f1_mnar_mar['point_delta']:+.4f}  CI95={res_f1_mnar_mar['ci95']}\")\n",
    "print(f\"Fcast3 RMSE: MNAR - MAR = {res_f3_mnar_mar['point_delta']:+.4f}  CI95={res_f3_mnar_mar['ci95']}\")\n",
    "print(f\"Fcast6 RMSE: MNAR - MAR = {res_f6_mnar_mar['point_delta']:+.4f}  CI95={res_f6_mnar_mar['ci95']}\")\n",
    "\n",
    "# --- Baseline sanity: SeasonalNaive should be compared to HourOfWeekMean ---\n",
    "res_f1_season_how = bootstrap_rmse_delta(season_sq1, how_sq1, n_boot=500, seed=21, mode=\"sqerr\")\n",
    "res_f3_season_how = bootstrap_rmse_delta(season_sq3, how_sq3, n_boot=500, seed=22, mode=\"sqerr\")\n",
    "res_f6_season_how = bootstrap_rmse_delta(season_sq6, how_sq6, n_boot=500, seed=23, mode=\"sqerr\")\n",
    "print(f\"\\nFcast1 RMSE: SeasonalNaive - HourOfWeek = {res_f1_season_how['point_delta']:+.4f}  CI95={res_f1_season_how['ci95']}\")\n",
    "print(f\"Fcast3 RMSE: SeasonalNaive - HourOfWeek = {res_f3_season_how['point_delta']:+.4f}  CI95={res_f3_season_how['ci95']}\")\n",
    "print(f\"Fcast6 RMSE: SeasonalNaive - HourOfWeek = {res_f6_season_how['point_delta']:+.4f}  CI95={res_f6_season_how['ci95']}\")\n",
    "\n",
    "# --- Models vs HourOfWeekMean ---\n",
    "res_f1_mar_how = bootstrap_rmse_delta(mar_sq1, how_sq1, n_boot=500, seed=31, mode=\"sqerr\")\n",
    "res_f3_mar_how = bootstrap_rmse_delta(mar_sq3, how_sq3, n_boot=500, seed=32, mode=\"sqerr\")\n",
    "res_f6_mar_how = bootstrap_rmse_delta(mar_sq6, how_sq6, n_boot=500, seed=33, mode=\"sqerr\")\n",
    "print(f\"\\nFcast1 RMSE: MAR - HourOfWeek = {res_f1_mar_how['point_delta']:+.4f}  CI95={res_f1_mar_how['ci95']}\")\n",
    "print(f\"Fcast3 RMSE: MAR - HourOfWeek = {res_f3_mar_how['point_delta']:+.4f}  CI95={res_f3_mar_how['ci95']}\")\n",
    "print(f\"Fcast6 RMSE: MAR - HourOfWeek = {res_f6_mar_how['point_delta']:+.4f}  CI95={res_f6_mar_how['ci95']}\")\n",
    "\n",
    "res_f1_mnar_how = bootstrap_rmse_delta(mnar_sq1, how_sq1, n_boot=500, seed=41, mode=\"sqerr\")\n",
    "res_f3_mnar_how = bootstrap_rmse_delta(mnar_sq3, how_sq3, n_boot=500, seed=42, mode=\"sqerr\")\n",
    "res_f6_mnar_how = bootstrap_rmse_delta(mnar_sq6, how_sq6, n_boot=500, seed=43, mode=\"sqerr\")\n",
    "print(f\"\\nFcast1 RMSE: MNAR - HourOfWeek = {res_f1_mnar_how['point_delta']:+.4f}  CI95={res_f1_mnar_how['ci95']}\")\n",
    "print(f\"Fcast3 RMSE: MNAR - HourOfWeek = {res_f3_mnar_how['point_delta']:+.4f}  CI95={res_f3_mnar_how['ci95']}\")\n",
    "print(f\"Fcast6 RMSE: MNAR - HourOfWeek = {res_f6_mnar_how['point_delta']:+.4f}  CI95={res_f6_mnar_how['ci95']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37713a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[HourOfWeekMean baseline] Forecast RMSEs:\n",
      "  1-step: 10.137130026808572\n",
      "  3-step: 9.579017766777703\n",
      "  6-step: 9.333722857457461\n",
      "\n",
      "=== Bootstrap RMSE deltas (delta = A - B; negative means A better) ===\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Hour-of-Week baseline ----------------\n",
    "# Build detector-specific mean by (day_of_week, hour) using TRAIN/OBSERVED values only.\n",
    "# Forecast fn signature must match: fn(x_t_masked, t_idx, detector_idx) -> float\n",
    "def _build_hour_of_week_stats(x_obs: np.ndarray, meta: dict):\n",
    "    # timestamps expected to be numpy datetime64 or pandas timestamps\n",
    "    ts = meta[\"timestamps\"]\n",
    "    # Convert to pandas to reliably get dayofweek/hour (works for datetime64 too)\n",
    "    ts_pd = pd.to_datetime(ts)\n",
    "    how = (ts_pd.dayofweek.to_numpy() * 24 + ts_pd.hour.to_numpy()).astype(int)  # 0..167\n",
    "\n",
    "    T, D = x_obs.shape\n",
    "    means = np.full((D, 168), np.nan, float)\n",
    "    global_means = np.nanmean(x_obs, axis=0)  # per-detector fallback\n",
    "\n",
    "    for d in range(D):\n",
    "        y = x_obs[:, d]\n",
    "        for k in range(168):\n",
    "            m = (how == k) & np.isfinite(y)\n",
    "            if np.any(m):\n",
    "                means[d, k] = float(np.mean(y[m]))\n",
    "    return means, global_means, how\n",
    "\n",
    "how_means, how_global_means, how_index = _build_hour_of_week_stats(x_t_train, meta)\n",
    "\n",
    "def hour_of_week_forecast_fn(x_t_masked: np.ndarray, t_idx: int, d: int) -> float:\n",
    "    k = int(how_index[t_idx])  # 0..167\n",
    "    v = how_means[d, k]\n",
    "    if np.isfinite(v):\n",
    "        return float(v)\n",
    "    # Fallback if that (d, hour-of-week) bin has no training samples\n",
    "    vg = how_global_means[d]\n",
    "    return float(vg) if np.isfinite(vg) else 0.0\n",
    "\n",
    "how_sq1 = collect_forecast_sqerr_baseline(x_t, x_t_train, meta, hour_of_week_forecast_fn, horizon=1)\n",
    "how_sq3 = collect_forecast_sqerr_baseline(x_t, x_t_train, meta, hour_of_week_forecast_fn, horizon=3)\n",
    "how_sq6 = collect_forecast_sqerr_baseline(x_t, x_t_train, meta, hour_of_week_forecast_fn, horizon=6)\n",
    "\n",
    "print(\"\\n[HourOfWeekMean baseline] Forecast RMSEs:\")\n",
    "print(\"  1-step:\", _rmse_from_sqerrs(how_sq1))\n",
    "print(\"  3-step:\", _rmse_from_sqerrs(how_sq3))\n",
    "print(\"  6-step:\", _rmse_from_sqerrs(how_sq6))\n",
    "\n",
    "# ---------- Bootstrap deltas ----------\n",
    "print(\"\\n=== Bootstrap RMSE deltas (delta = A - B; negative means A better) ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1be62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LinearInterp (impute) + SeasonalNaive (forecast)] Imputation performance:\n",
      "  MAE : 4.683692189112716\n",
      "  MSE : 55.559623086886994\n",
      "  RMSE: 7.4538327783018445\n",
      "\n",
      "[LinearInterp (impute) + SeasonalNaive (forecast)] Forecasting performance:\n",
      "-----------------------------------\n",
      "1-step MAE : 8.296791097902208\n",
      "1-step MSE : 190.43454001592664\n",
      "1-step RMSE: 13.799802173072143\n",
      "\n",
      "-----------------------------------\n",
      "3-step MAE : 7.088293768710437\n",
      "3-step MSE : 141.3627038084115\n",
      "3-step RMSE: 11.88960486342635\n",
      "\n",
      "-----------------------------------\n",
      "6-step MAE : 7.184098242292688\n",
      "6-step MSE : 142.20147969735766\n",
      "6-step RMSE: 11.924826191494686\n",
      "\n",
      "[LOCF (impute) + HourOfWeekMean (forecast)] Imputation performance:\n",
      "  MAE : 7.880999151920512\n",
      "  MSE : 181.6578376980285\n",
      "  RMSE: 13.478050218708509\n",
      "\n",
      "[LOCF (impute) + HourOfWeekMean (forecast)] Forecasting performance:\n",
      "-----------------------------------\n",
      "1-step MAE : 6.386332833952402\n",
      "1-step MSE : 96.49647383428665\n",
      "1-step RMSE: 9.823261873445432\n",
      "\n",
      "-----------------------------------\n",
      "3-step MAE : 5.880007466029467\n",
      "3-step MSE : 86.29808331896805\n",
      "3-step RMSE: 9.28967616868145\n",
      "\n",
      "-----------------------------------\n",
      "6-step MAE : 5.750116038082387\n",
      "6-step MSE : 78.87889217236422\n",
      "6-step RMSE: 8.881378956691591\n",
      "\n",
      "[LOCF (impute) + SeasonalNaive (forecast)] Imputation performance:\n",
      "  MAE : 7.880999151920512\n",
      "  MSE : 181.6578376980285\n",
      "  RMSE: 13.478050218708509\n",
      "\n",
      "[LOCF (impute) + SeasonalNaive (forecast)] Forecasting performance:\n",
      "-----------------------------------\n",
      "1-step MAE : 8.296791097902208\n",
      "1-step MSE : 190.43454001592664\n",
      "1-step RMSE: 13.799802173072143\n",
      "\n",
      "-----------------------------------\n",
      "3-step MAE : 7.088293768710437\n",
      "3-step MSE : 141.3627038084115\n",
      "3-step RMSE: 11.88960486342635\n",
      "\n",
      "-----------------------------------\n",
      "6-step MAE : 7.184098242292688\n",
      "6-step MSE : 142.20147969735766\n",
      "6-step RMSE: 11.924826191494686\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Baselines: Linear interp + Seasonal naive ----------------\n",
    "metrics_lin_season = evaluate_impute_forecast_baseline(\n",
    "    x_t_true=x_t,\n",
    "    x_t_masked=x_t_train,\n",
    "    meta=meta,\n",
    "    impute_fn=linear_interp_impute_baseline,\n",
    "    forecast_fn=seasonal_naive_forecast_baseline,\n",
    "    label=\"LinearInterp (impute) + SeasonalNaive (forecast)\",\n",
    ")\n",
    "\n",
    "\n",
    "how_mean, slot_of_week, global_mean = build_hour_of_week_climatology(\n",
    "    x_t_masked=x_t_train,\n",
    "    m_t_masked=m_t_train,\n",
    "    timestamps=meta[\"timestamps\"],\n",
    "    step_minutes=5,\n",
    ")\n",
    "hour_of_week_forecast_fn = make_hour_of_week_forecast_fn(how_mean, slot_of_week, global_mean)\n",
    "\n",
    "\n",
    "metrics_locf_how = evaluate_impute_forecast_baseline(\n",
    "    x_t_true=x_t,\n",
    "    x_t_masked=x_t_train,\n",
    "    meta=meta,\n",
    "    impute_fn=locf_impute_baseline,              # keep imputation baseline simple\n",
    "    forecast_fn=hour_of_week_forecast_fn,        # NEW baseline\n",
    "    label=\"LOCF (impute) + HourOfWeekMean (forecast)\",\n",
    ")\n",
    "\n",
    "metrics_locf_season = evaluate_impute_forecast_baseline(\n",
    "    x_t_true=x_t,\n",
    "    x_t_masked=x_t_train,\n",
    "    meta=meta,\n",
    "    impute_fn=locf_impute_baseline,\n",
    "    forecast_fn=seasonal_naive_forecast_baseline,\n",
    "    label=\"LOCF (impute) + SeasonalNaive (forecast)\",\n",
    ")\n",
    "\n",
    "metrics_spline_season = evaluate_impute_forecast_baseline(\n",
    "    x_t_true=x_t,\n",
    "    x_t_masked=x_t_train,\n",
    "    meta=meta,\n",
    "    impute_fn=spline_impute_baseline,\n",
    "    forecast_fn=seasonal_naive_forecast_baseline,\n",
    "    label=\"Spline (impute, fallback->linear) + SeasonalNaive (forecast)\",\n",
    ")\n",
    "\n",
    "def _delta(a, b, key):\n",
    "    return a[key] - b[key]\n",
    "\n",
    "print(\"\\n=== Summary deltas (lower is better) ===\")\n",
    "print(\"Dynamics win  (LOCF -> MAR)  impute_RMSE:\", _delta(metrics_mar, baseline_locf_metrics, \"impute_rmse\"))\n",
    "print(\"MNAR refine   (MAR  -> MNAR) impute_RMSE:\", _delta(metrics_mnar, metrics_mar, \"impute_rmse\"))\n",
    "print(\"MNAR refine   (MAR  -> MNAR) fcast_RMSE1:\", _delta(metrics_mnar, metrics_mar, \"forecast_rmse_1\"))\n",
    "print(\"MNAR refine   (MAR  -> MNAR) fcast_RMSE3:\", _delta(metrics_mnar, metrics_mar, \"forecast_rmse_3\"))\n",
    "print(\"MNAR refine   (MAR  -> MNAR) fcast_RMSE6:\", _delta(metrics_mnar, metrics_mar, \"forecast_rmse_6\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab4cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5) BRITS / GRU-D hook\n",
    "# ============================================================\n",
    "try:\n",
    "    # ------------------------------------------------------------\n",
    "    # GRU-D style imputer (torch-only baseline)\n",
    "    # - Causal imputation (uses past only)\n",
    "    # - Forecasting via free-run after blackout end (no peeking)\n",
    "    # ------------------------------------------------------------\n",
    "    class GRUDImputer(nn.Module):\n",
    "        def __init__(self, D: int, hidden: int = 128):\n",
    "            super().__init__()\n",
    "            self.D = D\n",
    "            self.hidden = hidden\n",
    "            # Per-feature decay -> D outputs\n",
    "            self.decay = nn.Linear(D, D)\n",
    "            # Input uses [x_tilde, obs_mask]\n",
    "            self.inp = nn.Linear(2 * D, hidden)\n",
    "            self.cell = nn.GRUCell(hidden, hidden)\n",
    "            self.out = nn.Linear(hidden, D)\n",
    "\n",
    "        def forward(\n",
    "            self,\n",
    "            x_filled: torch.Tensor,   # (B,T,D) NaNs already replaced\n",
    "            obs_mask: torch.Tensor,   # (B,T,D) 1 if observed else 0\n",
    "            delta: torch.Tensor,      # (B,T,D) time since last obs (in steps)\n",
    "            x_mean: torch.Tensor,     # (D,)\n",
    "            h0: torch.Tensor | None = None,  # (B,H)\n",
    "            last_x0: torch.Tensor | None = None,  # (B,D) initial last observed per feature\n",
    "        ):\n",
    "            B, T, D = x_filled.shape\n",
    "            device = x_filled.device\n",
    "\n",
    "            if h0 is None:\n",
    "                h = torch.zeros(B, self.hidden, device=device)\n",
    "            else:\n",
    "                h = h0\n",
    "\n",
    "            # last observed value per feature\n",
    "            if last_x0 is None:\n",
    "                last_x = x_mean[None, :].repeat(B, 1)  # (B,D)\n",
    "            else:\n",
    "                last_x = last_x0\n",
    "\n",
    "            preds = []\n",
    "            h_seq = []\n",
    "\n",
    "            for t in range(T):\n",
    "                x_t = x_filled[:, t, :]         # (B,D)\n",
    "                m_t = obs_mask[:, t, :]         # (B,D)\n",
    "                d_t = delta[:, t, :]            # (B,D)\n",
    "\n",
    "                # gamma = exp(-relu(W d + b))  in (0,1]\n",
    "                gamma = torch.exp(-torch.relu(self.decay(d_t)))\n",
    "\n",
    "                # GRU-D input imputation\n",
    "                x_hat = gamma * last_x + (1.0 - gamma) * x_mean[None, :]\n",
    "                x_tilde = m_t * x_t + (1.0 - m_t) * x_hat\n",
    "\n",
    "                # update last observed\n",
    "                last_x = m_t * x_t + (1.0 - m_t) * last_x\n",
    "\n",
    "                u = torch.tanh(self.inp(torch.cat([x_tilde, m_t], dim=-1)))  # (B,H)\n",
    "                h = self.cell(u, h)                                          # (B,H)\n",
    "                y = self.out(h)                                              # (B,D)\n",
    "\n",
    "                preds.append(y)\n",
    "                h_seq.append(h)\n",
    "\n",
    "            preds = torch.stack(preds, dim=1)   # (B,T,D)\n",
    "            h_seq = torch.stack(h_seq, dim=1)   # (B,T,H)\n",
    "            return preds, h_seq\n",
    "\n",
    "\n",
    "    def _compute_delta(obs_mask_np: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        obs_mask_np: (T,D) with 1 if observed else 0\n",
    "        returns delta in steps since last observation, (T,D)\n",
    "        \"\"\"\n",
    "        T, D = obs_mask_np.shape\n",
    "        delta = np.zeros((T, D), dtype=np.float32)\n",
    "        last = np.zeros(D, dtype=np.float32)\n",
    "        for t in range(T):\n",
    "            if t == 0:\n",
    "                delta[t] = 0.0\n",
    "            else:\n",
    "                last = last + 1.0\n",
    "                # reset where observed\n",
    "                last = last * (1.0 - obs_mask_np[t].astype(np.float32))\n",
    "                delta[t] = last\n",
    "        return delta\n",
    "\n",
    "\n",
    "    # -----------------------------\n",
    "    # Prepare tensors from your panel\n",
    "    # -----------------------------\n",
    "    # Expected existing vars in your notebook:\n",
    "    #   x_t_train, m_t_train, x_t, m_t, meta\n",
    "    # where m_t is 1=missing, 0=observed\n",
    "    x_train = x_t_train.copy()\n",
    "    obs_mask = ((m_t_train == 0) & np.isfinite(x_train)).astype(np.float32)  # (T,D) 1=observed\n",
    "    x_filled = np.nan_to_num(x_train, nan=0.0).astype(np.float32)            # (T,D)\n",
    "    delta = _compute_delta(obs_mask)                                         # (T,D)\n",
    "    x_mean = ( (x_filled * obs_mask).sum(axis=0) / (obs_mask.sum(axis=0) + 1e-6) ).astype(np.float32)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    D = x_filled.shape[1]\n",
    "    model_grud = GRUDImputer(D=D, hidden=128).to(device)\n",
    "    opt = torch.optim.Adam(model_grud.parameters(), lr=1e-3)\n",
    "\n",
    "    x_mean_t = torch.tensor(x_mean, device=device)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Mini-batch training via random subsequences\n",
    "    # -----------------------------\n",
    "    T_total = x_filled.shape[0]\n",
    "    seq_len = 288           # 1 day on 5-min grid\n",
    "    batch_size = 16\n",
    "    steps = 300             # keep modest; bump if you want\n",
    "\n",
    "    rs = np.random.default_rng(0)\n",
    "\n",
    "    def make_batch():\n",
    "        starts = rs.integers(0, max(1, T_total - seq_len - 1), size=batch_size)\n",
    "        xb = np.stack([x_filled[s:s+seq_len] for s in starts], axis=0)     # (B,L,D)\n",
    "        mb = np.stack([obs_mask[s:s+seq_len] for s in starts], axis=0)     # (B,L,D)\n",
    "        db = np.stack([delta[s:s+seq_len] for s in starts], axis=0)        # (B,L,D)\n",
    "        return xb, mb, db\n",
    "\n",
    "    model_grud.train()\n",
    "    for step in range(1, steps + 1):\n",
    "        xb, mb, db = make_batch()\n",
    "        xb_t = torch.tensor(xb, device=device)\n",
    "        mb_t = torch.tensor(mb, device=device)\n",
    "        db_t = torch.tensor(db, device=device)\n",
    "\n",
    "        pred, _ = model_grud(xb_t, mb_t, db_t, x_mean_t)\n",
    "\n",
    "        # loss only where observed (mb_t == 1)\n",
    "        diff2 = (pred - xb_t) ** 2\n",
    "        loss = (diff2 * mb_t).sum() / (mb_t.sum() + 1e-6)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_grud.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"[GRU-D] step {step:4d}/{steps}  loss={float(loss):.5f}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # One full forward pass to cache predictions + hidden states\n",
    "    # -----------------------------\n",
    "    model_grud.eval()\n",
    "    with torch.no_grad():\n",
    "        x_full = torch.tensor(x_filled[None, :, :], device=device)     # (1,T,D)\n",
    "        m_full = torch.tensor(obs_mask[None, :, :], device=device)     # (1,T,D)\n",
    "        d_full = torch.tensor(delta[None, :, :], device=device)        # (1,T,D)\n",
    "        pred_full, h_full = model_grud(x_full, m_full, d_full, x_mean_t)\n",
    "        pred_full = pred_full[0].cpu().numpy()                         # (T,D)\n",
    "        h_full = h_full[0].cpu().numpy()                               # (T,H)\n",
    "    \n",
    "    # Cache last observed value per feature at each timestep (for proper free-run)\n",
    "    last_x_hist = np.zeros_like(x_filled, dtype=np.float32)  # (T,D)\n",
    "    last = x_mean.astype(np.float32).copy()\n",
    "    for t in range(x_filled.shape[0]):\n",
    "        # update last where observed at time t\n",
    "        obs = obs_mask[t].astype(bool)\n",
    "        last[obs] = x_filled[t, obs]\n",
    "        last_x_hist[t] = last\n",
    "\n",
    "    # -----------------------------\n",
    "    # Forecast helper: free-run k steps after end_idx\n",
    "    # -----------------------------\n",
    "    def grud_forecast_k(end_idx: int, k: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns predicted x at time end_idx + k, without using ground truth\n",
    "        beyond end_idx (free-run with missing masks).\n",
    "        \"\"\"\n",
    "        model_grud.eval()\n",
    "        with torch.no_grad():\n",
    "            # start hidden at end_idx\n",
    "            h0 = torch.tensor(h_full[end_idx][None, :], device=device)  # (1,H)\n",
    "            last_x0 = torch.tensor(last_x_hist[end_idx][None, :], device=device)  # (1,D)\n",
    "\n",
    "            # build a tiny rollout of length k, with \"all missing\" inputs\n",
    "            # delta should start from per-feature delta at end_idx and then increase\n",
    "            xb = torch.zeros(1, k, D, device=device)\n",
    "            mb = torch.zeros(1, k, D, device=device)   # all missing\n",
    "\n",
    "            # delta should start from per-feature delta at end_idx and then increase\n",
    "            delta0 = torch.tensor(\n",
    "                delta[end_idx][None, :],\n",
    "                device=device,\n",
    "                dtype=torch.float32\n",
    "            )  # (1,D)\n",
    "            steps = torch.arange(\n",
    "                1, k + 1,\n",
    "                device=device,\n",
    "                dtype=torch.float32\n",
    "            )[:, None]  # (k,1)\n",
    "\n",
    "            # (1,k,D) — broadcasting does the right thing\n",
    "            db = (delta0 + steps).unsqueeze(0)\n",
    "\n",
    "            pred_k, _ = model_grud(xb, mb, db, x_mean_t, h0=h0, last_x0=last_x0)\n",
    "            return pred_k[0, -1].cpu().numpy()  # (D,)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Evaluate on the SAME windows using your existing evaluator\n",
    "    # -----------------------------\n",
    "    def grud_impute_fn(x_t_masked_unused, start_idx, end_idx, detector_idx):\n",
    "        # causal predictions from pred_full on the masked training panel\n",
    "        return pred_full[start_idx:end_idx+1, detector_idx].astype(float)\n",
    "\n",
    "    def grud_forecast_fn(x_t_masked_unused, target_idx, detector_idx):\n",
    "        # target_idx = end_idx + h  => forecast from end_idx, horizon=h\n",
    "        # Need end_idx; infer h by scanning forecast windows, or pass end_idx directly in a custom eval.\n",
    "        # Here, we use the fact that evaluator calls forecast_fn(target_idx, d) only.\n",
    "        # We'll approximate by using k=1 (NOT ideal). Prefer the custom eval below.\n",
    "        return float(pred_full[target_idx, detector_idx])\n",
    "\n",
    "    # Better: custom forecast evaluation that knows end_idx + horizon (no leakage)\n",
    "    def evaluate_grud_forecast_only(label=\"GRU-D (free-run forecast)\"):\n",
    "        y1t, y1p, y3t, y3p, y6t, y6p = [], [], [], [], [], []\n",
    "        forecast_windows = (\n",
    "            forecast_1_evaluation_windows_val\n",
    "            + forecast_3_evaluation_windows_val\n",
    "            + forecast_6_evaluation_windows_val\n",
    "        )\n",
    "        for w in forecast_windows:\n",
    "            end_idx = np.where(meta[\"timestamps\"] == w[\"blackout_end\"])[0][0]\n",
    "            d = np.where(meta[\"detectors\"] == w[\"detector_id\"])[0][0]\n",
    "            h = int(w[\"horizon_steps\"])\n",
    "            target_idx = end_idx + h\n",
    "            if target_idx >= x_t.shape[0]:\n",
    "                continue\n",
    "            yt = float(x_t[target_idx, d])\n",
    "            if not np.isfinite(yt):\n",
    "                continue\n",
    "            yp = float(grud_forecast_k(end_idx=end_idx, k=h)[d])\n",
    "            if not np.isfinite(yp):\n",
    "                continue\n",
    "            if h == 1:\n",
    "                y1t.append(yt); y1p.append(yp)\n",
    "            elif h == 3:\n",
    "                y3t.append(yt); y3p.append(yp)\n",
    "            elif h == 6:\n",
    "                y6t.append(yt); y6p.append(yp)\n",
    "\n",
    "        import sklearn.metrics\n",
    "        def _pack(y_pred, y_true, h):\n",
    "            mae = sklearn.metrics.mean_absolute_error(y_true, y_pred)\n",
    "            mse = sklearn.metrics.mean_squared_error(y_true, y_pred)\n",
    "            rmse = float(np.sqrt(mse))\n",
    "            print(f\"{h}-step MAE : {mae}\")\n",
    "            print(f\"{h}-step MSE : {mse}\")\n",
    "            print(f\"{h}-step RMSE: {rmse}\")\n",
    "\n",
    "        print(f\"\\n[{label}] Forecasting performance (free-run):\")\n",
    "        print(\"-----------------------------------\")\n",
    "        _pack(y1p, y1t, 1)\n",
    "        print(\"\\n-----------------------------------\")\n",
    "        _pack(y3p, y3t, 3)\n",
    "        print(\"\\n-----------------------------------\")\n",
    "        _pack(y6p, y6t, 6)\n",
    "\n",
    "\n",
    "    # Use your baseline evaluator for imputation (forecasting handled by custom function above)\n",
    "    metrics_grud_impute = evaluate_impute_forecast_baseline(\n",
    "        x_t_true=x_t,\n",
    "        x_t_masked=x_t_train,\n",
    "        meta=meta,\n",
    "        impute_fn=grud_impute_fn,\n",
    "        forecast_fn=seasonal_naive_forecast_baseline,  # keep a sane baseline here\n",
    "        label=\"GRU-D (impute via torch) + SeasonalNaive (forecast)\",\n",
    "    )\n",
    "    evaluate_grud_forecast_only(label=\"GRU-D (free-run forecast)\")\n",
    "\n",
    "    print(\"\\n[GRU-D] Done. You now have a torch baseline you can cite + compare.\")\n",
    "except Exception:\n",
    "    print(\"\\n[Optional] torch not available; skipping BRITS/GRU-D baseline hook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
